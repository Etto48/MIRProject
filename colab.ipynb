{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWsGxTnraW-Z"
      },
      "source": [
        "This notebook was automatically generated with our custom script (mir.scripts.ipynb_compiler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU5u8CY2aW-a"
      },
      "source": [
        "# Multimedia Information Retrieval - Project\n",
        "\n",
        "This project was developed by \"The Karate Kid\" team:\n",
        "\n",
        "- [Ettore Ricci](https://github.com/Etto48)\n",
        "- [Paolo Palumbo](https://github.com/paolpal)\n",
        "- [Zahra Omrani](https://github.com/zahra-omrani)\n",
        "- [Erni Deliallisi](https://github.com/erni-de)\n",
        "\n",
        "The whole codebase can be found on [GitHub](https://github.com/Etto48/MIRProject).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": false,
        "id": "zOx4qrPGaW-a",
        "outputId": "5d15e6eb-8fdf-4604-c73c-b54cdde6443b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: iprogress in /usr/local/lib/python3.10/dist-packages (0.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: more_itertools in /usr/local/lib/python3.10/dist-packages (10.5.0)\n",
            "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from iprogress) (1.17.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from python-terrier) (2.32.3)\n",
            "Requirement already satisfied: ir-datasets>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.9)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
            "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.13.1)\n",
            "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.6)\n",
            "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.8)\n",
            "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.12.3)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (2.5.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (5.3.0)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (4.3.3)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->python-terrier) (2024.12.14)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (3.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (1.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
            "fatal: destination path 'contriever' already exists and is not an empty directory.\n",
            "Obtaining file:///content/contriever\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beir in /usr/local/lib/python3.10/dist-packages (from src==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from src==0.1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from src==0.1.0) (4.47.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from beir->src==0.1.0) (3.3.1)\n",
            "Requirement already satisfied: pytrec-eval in /usr/local/lib/python3.10/dist-packages (from beir->src==0.1.0) (0.5)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from beir->src==0.1.0) (1.9.0.post1)\n",
            "Requirement already satisfied: elasticsearch==7.9.1 in /usr/local/lib/python3.10/dist-packages (from beir->src==0.1.0) (7.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from beir->src==0.1.0) (3.2.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir->src==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elasticsearch==7.9.1->beir->src==0.1.0) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->src==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->src==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->src==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->beir->src==0.1.0) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->src==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->src==0.1.0) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->src==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir->src==0.1.0) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir->src==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->beir->src==0.1.0) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->beir->src==0.1.0) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir->src==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir->src==0.1.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->beir->src==0.1.0) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir->src==0.1.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->beir->src==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->beir->src==0.1.0) (1.17.0)\n",
            "Building wheels for collected packages: src\n",
            "  Building editable for src (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for src: filename=src-0.1.0-0.editable-py3-none-any.whl size=15133 sha256=189c33f886c2633edbfb172bc0647b0490b279b78f42e0cb141e78fa9a370252\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cv06vy6c/wheels/56/bb/cf/c177ac9deab8a3be8a77a89f9eeb27836d32bfe2c2e961775a\n",
            "Successfully built src\n",
            "Installing collected packages: src\n",
            "  Attempting uninstall: src\n",
            "    Found existing installation: src 0.1.0\n",
            "    Uninstalling src-0.1.0:\n",
            "      Successfully uninstalled src-0.1.0\n",
            "Successfully installed src-0.1.0\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "%pip install pandas tqdm iprogress ipywidgets unidecode nltk more_itertools python-terrier torch transformers psutil\n",
        "!git clone https://github.com/facebookresearch/contriever\n",
        "!echo -e '[project]\\nname = \"src\"\\nversion = \"0.1.0\"\\ndescription = \"contriever\"\\ndependencies = [\"beir\", \"torch\", \"transformers\",]\\n\\n[project.license]\\nfile = \"LICENSE\"\\n\\n[tool.setuptools.package-dir]\\nsrc = \"src\"' > contriever/pyproject.toml\n",
        "!pip install -e contriever/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": false,
        "id": "5AmxU4L1aW-c"
      },
      "outputs": [],
      "source": [
        "# define __file__ and set env variable\n",
        "import os\n",
        "__file__ = os.path.abspath('colab.ipynb')\n",
        "os.environ['MIR_NOTEBOOK'] = __file__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": false,
        "id": "ZRXe65LOaW-c"
      },
      "outputs": [],
      "source": [
        "#%% === mir ===\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "pd.options.mode.copy_on_write = True\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    COLAB = True\n",
        "else:\n",
        "    COLAB = False\n",
        "\n",
        "if COLAB or os.getenv(\"MIR_NOTEBOOK\") is not None:\n",
        "    PROJECT_DIR = os.path.abspath(\"./\")\n",
        "else:\n",
        "    PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.mkdir(DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": false,
        "id": "x897k7KbaW-d"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.document_contents ===\n",
        "\n",
        "\n",
        "\n",
        "class DocumentContents:\n",
        "    def __init__(self, author: str, title: str, body: str, **kwargs):\n",
        "        self.author = author\n",
        "        self.title = title\n",
        "        self.body = body\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "    def add_field(self, field: str, value: str):\n",
        "        self.__dict__[field] = value\n",
        "\n",
        "    def set_score(self, score: float):\n",
        "        self.score = score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": false,
        "id": "NBj8g1V7aW-e"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.token_ir ===\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "class TokenLocation(Enum) :\n",
        "    QUERY = 0\n",
        "    AUTHOR = 1\n",
        "    TITLE = 2\n",
        "    BODY = 3\n",
        "\n",
        "@dataclass\n",
        "class Token:\n",
        "    text: str\n",
        "    location: TokenLocation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": false,
        "id": "nV_IueFRaW-e"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.tokenizer ===\n",
        "\n",
        "from abc import abstractmethod\n",
        "from typing import Protocol\n",
        "\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.token_ir import Token\n",
        "\n",
        "\n",
        "class Tokenizer(Protocol):\n",
        "    @abstractmethod\n",
        "    def tokenize_query(self, query: str) -> list[Token]:\n",
        "        \"\"\"\n",
        "        Tokenize a query.\n",
        "\n",
        "        # Parameters\n",
        "        - query (str): The query to tokenize.\n",
        "\n",
        "        # Returns\n",
        "        - list[Token]: The tokens of the query.\n",
        "        \"\"\"\n",
        "    @abstractmethod\n",
        "    def tokenize_document(self, doc: DocumentContents) -> list[Token]:\n",
        "        \"\"\"\n",
        "        Tokenize a document.\n",
        "\n",
        "        # Parameters\n",
        "        - doc (DocumentContents): The document to tokenize.\n",
        "\n",
        "        # Returns\n",
        "        - list[Token]: The tokens of the document.\n",
        "        \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": false,
        "id": "oHvZGzMXaW-f"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.document_info ===\n",
        "\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.token_ir import TokenLocation\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "\n",
        "\n",
        "class DocumentInfo:\n",
        "    def __init__(self, id: int, lengths: list[int]):\n",
        "        assert len(lengths) == 3, \"Lengths must have 3 elements, [author, title, body]\"\n",
        "        self.id = id\n",
        "        self.lengths = lengths\n",
        "\n",
        "    @staticmethod\n",
        "    def from_document_contents(id: int, doc: DocumentContents, tokenizer: Tokenizer) -> \"DocumentInfo\":\n",
        "        tokens = tokenizer.tokenize_document(doc)\n",
        "        tokens_for_field = [0,0,0]\n",
        "        for token in tokens:\n",
        "            match token.location:\n",
        "                case TokenLocation.AUTHOR:\n",
        "                    field_offset = 0\n",
        "                case TokenLocation.TITLE:\n",
        "                    field_offset = 1\n",
        "                case TokenLocation.BODY:\n",
        "                    field_offset = 2\n",
        "                case _:\n",
        "                    raise ValueError(f\"Invalid token location {token.location}\")\n",
        "            tokens_for_field[field_offset] += 1\n",
        "        return DocumentInfo(id, tokens_for_field)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": false,
        "id": "4Coe3Un8aW-h"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.posting ===\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "class Posting:\n",
        "    def __init__(self, doc_id: int, term_id: int, occurrences: Optional[dict[str, int]] = None):\n",
        "        self.term_id = term_id\n",
        "        self.doc_id = doc_id\n",
        "        self.occurrences = occurrences if occurrences is not None else {\"author\": 0, \"title\": 0, \"body\": 0}\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Posting(doc_id={self.doc_id}, term_id={self.term_id}, occurrences={self.occurrences})\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": false,
        "id": "4O5vjUJZaW-i"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.term ===\n",
        "\n",
        "class Term:\n",
        "    def __init__(self, term: str, id: int, **kwargs):\n",
        "        self.term = term\n",
        "        self.id = id\n",
        "        self.info = kwargs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": false,
        "id": "WjmBNPaHaW-j"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.scoring_function ===\n",
        "\n",
        "from typing import Any, Callable, Optional, Protocol\n",
        "\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.term import Term\n",
        "\n",
        "\n",
        "class ScoringFunction(Protocol):\n",
        "    batched_call: Optional[Callable[[\"ScoringFunction\",list[str],str], list[float]]] = None\n",
        "    def __call__(self, document_info: DocumentInfo, postings: list[Posting], query: list[Term], **kwargs: dict[str, Any]) -> float:\n",
        "        \"\"\"\n",
        "        Score a document based on the postings and the query.\n",
        "\n",
        "        # Parameters\n",
        "        - document_info (DocumentInfo): The document info relative to the document to score.\n",
        "        - postings (list[Posting]): The postings related to the document and the query.\n",
        "        - query (list[Term]): The query terms.\n",
        "        - **kwargs (dict[str, Any]): Additional arguments for the scoring function.\n",
        "\n",
        "        # Returns\n",
        "        - float: The score of the document.\n",
        "        \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": false,
        "id": "GSnP7lpsaW-k"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.bm25f_scoring ===\n",
        "\n",
        "from typing import List, Dict\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.scoring_function import ScoringFunction\n",
        "# from mir.ir.term import Term\n",
        "import math\n",
        "\n",
        "\n",
        "class BM25FScoringFunction(ScoringFunction):\n",
        "    def __init__(self, k1: float = 1.5, b: float = 0.75, field_weights: Dict[str, float] = None):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.field_weights = field_weights if field_weights is not None else {'title': 2.0, 'body': 1.0, 'author': 0.5}\n",
        "\n",
        "    def _build_postings_dict(self, postings: List[Posting]) -> Dict[int, Posting]:\n",
        "        return {posting.term_id: posting for posting in postings}\n",
        "\n",
        "    def __call__(self, document: DocumentInfo, postings: List[Posting], query: List[Term], *, num_docs: int, avg_field_lengths: dict[str, int], **_) -> float:\n",
        "        postings_dict = self._build_postings_dict(postings)\n",
        "        score = 0.0\n",
        "        for term in query:\n",
        "            if term.id in postings_dict:\n",
        "                score += self._rsv(term, document, num_docs, postings_dict, avg_field_lengths)\n",
        "        return score\n",
        "\n",
        "    def _rsv(self, term: Term, document: DocumentInfo, num_docs: int, postings_dict: dict[int, Posting], avg_field_lengths: dict[str, int]) -> float:\n",
        "        tfd = self._wtf(term, document, postings_dict, avg_field_lengths)\n",
        "        idf = math.log(num_docs / term.info['document_frequency'])\n",
        "\n",
        "        if tfd > 0:\n",
        "            return (tfd / (self.k1 + tfd)) * idf\n",
        "        return 0.0\n",
        "\n",
        "    def _wtf(self, term: Term, document: DocumentInfo, postings_dict: dict[int, Posting], avg_field_lengths: dict[str, int]) -> float:\n",
        "        tfd = 0.0\n",
        "        field_indices = {\"author\": 0, \"title\": 1, \"body\": 2}\n",
        "\n",
        "        if term.id not in postings_dict:\n",
        "            return 0.0\n",
        "\n",
        "        posting = postings_dict[term.id]\n",
        "        for field, weight in self.field_weights.items():\n",
        "            tf = posting.occurrences.get(field, 0)\n",
        "            if tf == 0:\n",
        "                continue\n",
        "            field_index = field_indices[field]\n",
        "            avg_dlf = avg_field_lengths[field]\n",
        "            bb = 1 - self.b + self.b * document.lengths[field_index] / avg_dlf\n",
        "            tfd += weight * tf / bb\n",
        "\n",
        "        return tfd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": false,
        "id": "3QAy3recaW-k"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.count_scoring_function ===\n",
        "\n",
        "# from mir.ir.scoring_function import ScoringFunction\n",
        "\n",
        "\n",
        "class CountScoringFunction(ScoringFunction):\n",
        "    def __call__(self, document, postings, query, **kwargs):\n",
        "        return len(postings) / len(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": false,
        "id": "1d5pVKr_aW-k"
      },
      "outputs": [],
      "source": [
        "#%% === mir.utils.sized_generator ===\n",
        "\n",
        "from collections.abc import Generator, Sized\n",
        "from typing import TypeVar, Generic\n",
        "\n",
        "T = TypeVar('T')\n",
        "P = TypeVar('P')\n",
        "Q = TypeVar('Q')\n",
        "\n",
        "\n",
        "class SizedGenerator(Generic[T, P, Q], Generator[T, P, Q], Sized):\n",
        "    def __init__(self, generator: Generator[T, P, Q], length: int):\n",
        "        self.generator = generator\n",
        "        self.length = length\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def send(self, value):\n",
        "        return self.generator.send(value)\n",
        "\n",
        "    def throw(self, typ, val=None, tb=None):\n",
        "        return self.generator.throw(typ, val, tb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqNfNQMaW-l"
      },
      "source": [
        "# Index\n",
        "\n",
        "This interface is the component that actually holds the inverted index and exposes the methods to interact with it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": false,
        "id": "cLbp7HjcaW-l"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.index ===\n",
        "\n",
        "from abc import abstractmethod\n",
        "from collections.abc import Generator\n",
        "from typing import Any, Optional, Protocol\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.term import Term\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "# from mir.utils.sized_generator import SizedGenerator\n",
        "\n",
        "\n",
        "class Index(Protocol):\n",
        "    def get_global_info(self) -> dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get global info from the index.\n",
        "\n",
        "        # Returns\n",
        "        - dict[str, int]: A dictionary with global info.\n",
        "        \"\"\"\n",
        "        return {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
        "        \"\"\"\n",
        "        Get a generator of postings for a term_id.\n",
        "        MUST be sorted by doc_id.\n",
        "\n",
        "        # Parameters\n",
        "        - term_id (int): The term_id.\n",
        "\n",
        "        # Yields\n",
        "        - Posting: A posting from the posting list related to the term_id.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
        "        \"\"\"\n",
        "        Get document info from a doc_id.\n",
        "\n",
        "        # Parameters\n",
        "        - doc_id (int): The doc_id.\n",
        "\n",
        "        # Returns\n",
        "        - DocumentInfo: The document info related to the doc_id.\n",
        "        \"\"\"\n",
        "\n",
        "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
        "        \"\"\"\n",
        "        Get document contents from a doc_id.\n",
        "\n",
        "        # Parameters\n",
        "        - doc_id (int): The doc_id.\n",
        "\n",
        "        # Returns\n",
        "        - DocumentContents: The document contents related to the doc_id.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_term(self, term_id: int) -> Term:\n",
        "        \"\"\"\n",
        "        Get term info from a term_id.\n",
        "\n",
        "        # Parameters\n",
        "        - term_id (int): The term_id.\n",
        "\n",
        "        # Returns\n",
        "        - Term: The term related to the term_id.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_term_id(self, term: str) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Get term_id from a term in string format.\n",
        "        Returns None if the term is not in the index.\n",
        "\n",
        "        # Parameters\n",
        "        - term (str): The term in string format.\n",
        "\n",
        "        # Returns\n",
        "        - Optional[int]: The term_id related to the term or None if the term is not in the index.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of documents in the index.\n",
        "\n",
        "        # Returns\n",
        "        - int: The number of documents in the index.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
        "        \"\"\"\n",
        "        Add a document to the index.\n",
        "\n",
        "        # Parameters\n",
        "        - doc (DocumentContents): The document to add to the index.\n",
        "        - tokenizer (Tokenizer): The tokenizer to use to tokenize the document.\n",
        "        \"\"\"\n",
        "\n",
        "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], tokenizer: Tokenizer, verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Add multiple documents to the index, this calls index_document for each document.\n",
        "\n",
        "        # Parameters\n",
        "        - docs (SizedGenerator[DocumentContents, None, None]): A generator of documents to add to the index.\n",
        "        - tokenizer (Tokenizer): The tokenizer to use to tokenize the documents.\n",
        "        - verbose (bool): Whether to show a progress bar.\n",
        "        \"\"\"\n",
        "        for doc in tqdm(docs, desc=\"Indexing documents\", disable=not verbose, total=len(docs)):\n",
        "            self.index_document(doc, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": false,
        "id": "BEtGA2vMaW-l"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.default_index ===\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections.abc import Generator\n",
        "# import os\n",
        "import pickle\n",
        "from typing import Any, Optional\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.index import Index\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.term import Term\n",
        "# from mir.ir.token_ir import TokenLocation\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "# from mir.utils.sized_generator import SizedGenerator\n",
        "\n",
        "\n",
        "class DefaultIndex(Index):\n",
        "    def __init__(self, path: Optional[str] = None):\n",
        "        super().__init__()\n",
        "        self.postings: list[OrderedDict[Posting]] = []\n",
        "        self.document_info: list[DocumentInfo] = []\n",
        "        self.document_contents: list[DocumentContents] = []\n",
        "        self.terms: list[Term] = []\n",
        "        self.term_lookup: dict[str, int] = {}\n",
        "        self.path = None\n",
        "        self.total_field_lengths = {\n",
        "            \"author\": 0,\n",
        "            \"title\": 0,\n",
        "            \"body\": 0\n",
        "        }\n",
        "        if path is not None:\n",
        "            self.path = path\n",
        "            if os.path.exists(path):\n",
        "                self.load()\n",
        "\n",
        "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
        "        for doc_id, posting in self.postings[term_id].items():\n",
        "            yield posting\n",
        "\n",
        "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
        "        return self.document_info[doc_id]\n",
        "\n",
        "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
        "        return self.document_contents[doc_id]\n",
        "\n",
        "    def get_term(self, term_id: int) -> Term:\n",
        "        return self.terms[term_id]\n",
        "\n",
        "    def get_term_id(self, term: str) -> Optional[int]:\n",
        "        return self.term_lookup.get(term)\n",
        "\n",
        "    def get_global_info(self) -> dict[str, Any]:\n",
        "        return {\n",
        "            \"avg_field_lengths\": {\n",
        "                \"author\": self.total_field_lengths[\"author\"] / len(self.document_info),\n",
        "                \"title\": self.total_field_lengths[\"title\"] / len(self.document_info),\n",
        "                \"body\": self.total_field_lengths[\"body\"] / len(self.document_info)\n",
        "            },\n",
        "            \"num_docs\": len(self.document_info)\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.document_info)\n",
        "\n",
        "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
        "        terms = tokenizer.tokenize_document(doc)\n",
        "        author_length = sum(1 for term in terms if term.location == TokenLocation.AUTHOR)\n",
        "        title_length = sum(1 for term in terms if term.location == TokenLocation.TITLE)\n",
        "        body_length = sum(1 for term in terms if term.location == TokenLocation.BODY)\n",
        "        self.total_field_lengths[\"author\"] += author_length\n",
        "        self.total_field_lengths[\"title\"] += title_length\n",
        "        self.total_field_lengths[\"body\"] += body_length\n",
        "        term_ids = []\n",
        "        for term in terms:\n",
        "            if term.text not in self.term_lookup:\n",
        "                term_id = len(self.terms)\n",
        "                self.terms.append(Term(term.text, term_id))\n",
        "                self.term_lookup[term.text] = term_id\n",
        "            else:\n",
        "                term_id = self.term_lookup[term.text]\n",
        "            term_ids.append(term_id)\n",
        "        doc_id = len(self.document_info)\n",
        "        self.document_info.append(DocumentInfo.from_document_contents(doc_id, doc, tokenizer))\n",
        "        self.document_contents.append(doc)\n",
        "        for term_id in term_ids:\n",
        "            if term_id >= len(self.postings):\n",
        "                self.postings.append(OrderedDict())\n",
        "            self.postings[term_id][doc_id] = Posting(doc_id, term_id)\n",
        "\n",
        "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], tokenizer: Tokenizer, verbose: bool = False) -> None:\n",
        "        super().bulk_index_documents(docs, tokenizer, verbose)\n",
        "        if self.path is not None:\n",
        "            self.save()\n",
        "\n",
        "    def load(self):\n",
        "        if self.path is not None:\n",
        "            try:\n",
        "                with open(self.path, \"rb\") as f:\n",
        "                    postings, document_info, document_contents, terms, term_lookup = pickle.load(f)\n",
        "                assert isinstance(postings, list)\n",
        "                assert isinstance(document_info, list)\n",
        "                assert isinstance(document_contents, list)\n",
        "                assert isinstance(terms, list)\n",
        "                assert isinstance(term_lookup, dict)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "            else:\n",
        "                self.postings = postings\n",
        "                self.document_info = document_info\n",
        "                self.document_contents = document_contents\n",
        "                self.terms = terms\n",
        "                self.term_lookup = term_lookup\n",
        "        else:\n",
        "            raise ValueError(\"Path not set for index.\")\n",
        "\n",
        "    def save(self):\n",
        "        if self.path is not None:\n",
        "            with open(self.path, \"wb\") as f:\n",
        "                pickle.dump((self.postings, self.document_info, self.document_contents, self.terms, self.term_lookup), f)\n",
        "        else:\n",
        "            raise ValueError(\"Path not set for index.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": false,
        "id": "MLNJ4CnGaW-l"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.default_tokenizers ===\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "import unidecode\n",
        "# from mir import DATA_DIR\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "# from mir.ir.token_ir import Token, TokenLocation\n",
        "\n",
        "\n",
        "class DefaultTokenizer(Tokenizer):\n",
        "    def __init__(self):\n",
        "        download_dir = f\"{DATA_DIR}/nltk_data\"\n",
        "        nltk.download(\"stopwords\", quiet=True, download_dir=download_dir,)\n",
        "        stopwords_from_path = nltk.data.find(\"corpora/stopwords/english\", [download_dir])\n",
        "        with open(stopwords_from_path) as f:\n",
        "            self.stopwords = frozenset(f.read().splitlines())\n",
        "\n",
        "        self.stemmer = nltk.SnowballStemmer(\"english\")\n",
        "        self.remove_punctuation = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
        "        self.separate_numbers = str.maketrans({key: f\" {key} \" for key in string.digits})\n",
        "\n",
        "    def preprocess(self, text: str):\n",
        "        # normalize unicode\n",
        "        text = unidecode.unidecode(text, errors=\"replace\", replace_str=\" \")\n",
        "        # replace punctuation with space\n",
        "        text = text.translate(self.remove_punctuation).lower()\n",
        "        # separate numbers with a space\n",
        "        text = text.translate(self.separate_numbers)\n",
        "        # split text into words\n",
        "        words = text.split()\n",
        "        # remove stopwords\n",
        "        words = [word for word in words if word not in self.stopwords]\n",
        "        # stem words\n",
        "        words: list[str] = [self.stemmer.stem(word) for word in words]\n",
        "        return words\n",
        "\n",
        "\n",
        "    def tokenize_query(self, query: str) -> list[Token]:\n",
        "        query_list = self.preprocess(query)\n",
        "        token_list = [Token(word, TokenLocation.QUERY) for word in query_list]\n",
        "\n",
        "        return token_list\n",
        "\n",
        "    def tokenize_document(self, doc: DocumentContents) -> list[Token]:\n",
        "        author_list = self.preprocess(doc.author)\n",
        "        title_list = self.preprocess(doc.title)\n",
        "        body_list = self.preprocess(doc.body)\n",
        "\n",
        "        token_list = \\\n",
        "            [Token(aword, TokenLocation.AUTHOR) for aword in author_list] + \\\n",
        "            [Token(tword, TokenLocation.TITLE) for tword in title_list] + \\\n",
        "            [Token(bword, TokenLocation.BODY) for bword in body_list]\n",
        "\n",
        "        return token_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": false,
        "id": "JNMdo-K0aW-m"
      },
      "outputs": [],
      "source": [
        "#%% === mir.neural_relevance.dataset ===\n",
        "\n",
        "from typing import Literal\n",
        "import torch\n",
        "from torch import nn\n",
        "# import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from mir import DATA_DIR\n",
        "\n",
        "class MSMarcoDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, collection_path: str, queries_path: str, qrels_path: str):\n",
        "        self.collection = self.load_collection(collection_path)\n",
        "        self.queries = self.load_queries(queries_path)\n",
        "        self.qrels = self.load_qrels(qrels_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(mode: Literal[\"train\", \"valid\", \"test\"]):\n",
        "        collection_path = f\"{DATA_DIR}/msmarco/collection.tsv\"\n",
        "        match mode:\n",
        "            case \"train\":\n",
        "                queries_path = f\"{DATA_DIR}/msmarco/queries.train.tsv\"\n",
        "                qrels_path = f\"{DATA_DIR}/msmarco/qrels.train.tsv\"\n",
        "            case \"valid\":\n",
        "                queries_path = f\"{DATA_DIR}/msmarco/msmarco-test2019-queries.tsv\"\n",
        "                qrels_path = f\"{DATA_DIR}/msmarco/2019qrels-pass.txt\"\n",
        "            case \"test\":\n",
        "                raise NotImplementedError(f\"Mode {mode} not implemented.\")\n",
        "            case _:\n",
        "                raise ValueError(f\"Invalid mode {mode}.\")\n",
        "        return MSMarcoDataset(collection_path, queries_path, qrels_path)\n",
        "\n",
        "    def load_collection(self, collection_path: str):\n",
        "        collection = pd.read_csv(collection_path, sep='\\t', header=None, names=['docid', 'text'], index_col='docid')\n",
        "        return collection\n",
        "\n",
        "    def load_queries(self, queries_path: str):\n",
        "        queries = pd.read_csv(queries_path, sep='\\t', header=None, names=['qid', 'text'], index_col='qid')\n",
        "        return queries\n",
        "\n",
        "    def load_qrels(self, qrels_path: str):\n",
        "        sep = ' ' if qrels_path.endswith(\".txt\") else '\\t'\n",
        "        qrels = pd.read_csv(qrels_path, sep=sep, header=None, names=['qid', 'Q0', 'docid', 'relevance'])\n",
        "        return qrels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.qrels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        qid = self.qrels.iloc[idx]['qid']\n",
        "        docid = self.qrels.iloc[idx]['docid']\n",
        "        relevance = self.qrels.iloc[idx]['relevance']\n",
        "        query = self.queries.loc[qid]['text']\n",
        "        doc = self.collection.loc[docid]['text']\n",
        "        return query, doc, relevance\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        queries, docs, relevances = zip(*batch)\n",
        "        return queries, docs, torch.tensor(relevances, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Jmlu1RMUaW-m",
        "outputId": "291b8563-398a-4dfe-b8de-10003fb69374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/contriever-msmarco were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.7735, 1.5486, 1.5952], grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# import os\n",
        "import requests\n",
        "# import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "import transformers\n",
        "from src.contriever import Contriever\n",
        "\n",
        "contriever_ = Contriever.from_pretrained(\"facebook/contriever-msmarco\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/contriever-msmarco\")\n",
        "\n",
        "sentences = [\n",
        "    \"Where was Marie Curie born?\",\n",
        "    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n",
        "    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eugne Curie, a doctor of French Catholic origin from Alsace.\"\n",
        "]\n",
        "\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "embeddings = contriever_(**inputs)\n",
        "\n",
        "x = torch.triu(embeddings @ embeddings.T, diagonal = 1)\n",
        "x = x[x != 0]\n",
        "x.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": false,
        "id": "ex4hm2YsaW-m"
      },
      "outputs": [],
      "source": [
        "#%% === mir.neural_relevance.model ===\n",
        "\n",
        "# import os\n",
        "import requests\n",
        "#import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "import transformers\n",
        "\n",
        "# from mir import DATA_DIR\n",
        "# from mir.neural_relevance.dataset import MSMarcoDataset\n",
        "\n",
        "class NeuralRelevance(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        model_name = \"facebook-contriever\"\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/contriever-msmarco\")\n",
        "        self.model = Contriever.from_pretrained(\"facebook/contriever-msmarco\").to(self.device)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        bert_embedding_size = self.model.config.hidden_size\n",
        "        self.similairty_head = nn.Sequential(\n",
        "            nn.Linear(bert_embedding_size, 1, device=self.device),\n",
        "            nn.Sigmoid()\n",
        "        ).to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, q_tokens: dict, d_tokens: dict) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            query_embeddings = self.model(**q_tokens)\n",
        "            doc_embeddings = self.model(**d_tokens)\n",
        "        x = torch.triu(query_embeddings @ doc_embeddings.T, diagonal=1)\n",
        "        x = x[x != 0]\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def preprocess(self, queries: list[str], documents: list[str]):\n",
        "        q_tokens = self.tokenizer(queries, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        d_tokens = self.tokenizer(documents, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
        "        return q_tokens, d_tokens\n",
        "\n",
        "    def forward_queries_and_documents(self, queries: list[str], documents: list[str]) -> torch.Tensor:\n",
        "        q_tokens, d_tokens = self.preprocess(queries, documents)\n",
        "        return self.forward(q_tokens, d_tokens)\n",
        "\n",
        "    def save(self, path: str):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def return_model(self):\n",
        "      return self.model\n",
        "\n",
        "    @staticmethod\n",
        "    def from_pretrained():\n",
        "        if not os.path.exists(f\"{DATA_DIR}/neural_relevance.pt\"):\n",
        "            url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/neural_relevance.pt\"\n",
        "            weights_request = requests.get(url)\n",
        "            weights_request.raise_for_status()\n",
        "            with tqdm(total=int(weights_request.headers[\"Content-Length\"]), unit=\"B\", unit_scale=True, desc=\"Downloading weights\") as pbar:\n",
        "                with open(f\"{DATA_DIR}/neural_relevance.pt\", \"wb\") as f:\n",
        "                    for chunk in weights_request.iter_content(chunk_size=1024):\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "        model = NeuralRelevance.load(f\"{DATA_DIR}/neural_relevance.pt\")\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": false,
        "id": "tKtKFiySaW-m"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.neural_scoring_function ===\n",
        "\n",
        "import numpy as np\n",
        "# import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from mir import DATA_DIR\n",
        "# from mir.neural_relevance.model import NeuralRelevance\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.scoring_function import ScoringFunction\n",
        "# from mir.ir.term import Term\n",
        "# from mir.neural_relevance.dataset import MSMarcoDataset\n",
        "\n",
        "\n",
        "class NeuralScoringFunction(ScoringFunction):\n",
        "    def __init__(self):\n",
        "        # Load the model\n",
        "        self.model = NeuralRelevance()\n",
        "        self.model.eval()\n",
        "\n",
        "    def __call__(self, document: DocumentInfo, postings: list[Posting], query: list[Term], *, document_content: str, query_content: str, **kwargs) -> float:\n",
        "        if len(document_content) == 0 or len(query_content) == 0:\n",
        "            return 0.0\n",
        "        with torch.no_grad():\n",
        "            score = self.model.forward_queries_and_documents([query_content], [document_content])\n",
        "        return score.item()\n",
        "    \"\"\"\n",
        "    def batched_call(self, document_contents: list[str], query_contents: str) -> list[float]:\n",
        "        scores = []\n",
        "        with torch.no_grad():\n",
        "            scores = self.model.forward_queries_and_documents([query_contents]*len(document_contents), document_contents)\n",
        "        return scores.tolist()\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": false,
        "id": "g-cJS7QnaW-n"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.impls.sqlite_index ===\n",
        "\n",
        "from collections.abc import Generator\n",
        "# import os\n",
        "import sqlite3\n",
        "import sys\n",
        "from typing import Any, Optional\n",
        "\n",
        "import psutil\n",
        "# from mir.ir.document_info import DocumentInfo\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.impls.default_tokenizers import DefaultTokenizer\n",
        "# from mir.ir.index import Index\n",
        "# from mir.ir.posting import Posting\n",
        "# from mir.ir.term import Term\n",
        "# from mir.ir.token_ir import TokenLocation\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "\n",
        "\n",
        "class SqliteIndex(Index):\n",
        "    def __init__(self, path: Optional[str] = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.connection = sqlite3.connect(\n",
        "            path if path is not None else \":memory:\",\n",
        "            check_same_thread=False,\n",
        "            cached_statements=1024,)\n",
        "\n",
        "        assert sys.version_info.major == 3, \"Python 2 is not supported\"\n",
        "        assert sys.version_info.minor >= 10, \"Python <3.10 is not supported\"\n",
        "\n",
        "        legacy_mode = sys.version_info.minor == 10\n",
        "        if legacy_mode:\n",
        "            self.isolation_level = None\n",
        "        else:\n",
        "            self.connection.autocommit = True\n",
        "\n",
        "        self.connection.execute(\"pragma synchronous = off\")\n",
        "        self.connection.execute(f\"pragma threads = {os.cpu_count()}\")\n",
        "        self.connection.execute(\"pragma journal_mode = WAL\")\n",
        "        cache_memory = psutil.virtual_memory().total // 1024 // 2\n",
        "        self.connection.execute(f\"pragma cache_size = {-cache_memory}\")\n",
        "        self.connection.execute(f\"pragma mmap_size = {1024*1024*1024 * 16}\")\n",
        "        self.connection.execute(\"pragma temp_store = memory\")\n",
        "\n",
        "        if legacy_mode:\n",
        "            self.connection.isolation_level = \"DEFERRED\"\n",
        "        else:\n",
        "            self.connection.autocommit = False\n",
        "\n",
        "\n",
        "        self.connection.execute(\n",
        "            \"create table if not exists postings \"\n",
        "            \"(term_id integer references terms(term_id) not null, \"\n",
        "            \"doc_id integer references document_info(doc_id) not null, \"\n",
        "            \"occurrences_author integer not null, \"\n",
        "            \"occurrences_title integer not null, \"\n",
        "            \"occurrences_body integer not null, \"\n",
        "            \"primary key (term_id, doc_id))\")\n",
        "        self.connection.execute(\n",
        "            \"create table if not exists document_info \"\n",
        "            \"(doc_id integer not null primary key autoincrement, \"\n",
        "            \"author_len integer not null, \"\n",
        "            \"title_len integer not null, \"\n",
        "            \"body_len integer not null)\")\n",
        "        self.connection.execute(\n",
        "            \"create table if not exists document_contents \"\n",
        "            \"(doc_id integer not null primary key references document_info(doc_id), \"\n",
        "            \"author text, \"\n",
        "            \"title text, \"\n",
        "            \"body text)\")\n",
        "        self.connection.execute(\n",
        "            \"create table if not exists terms \"\n",
        "            \"(term_id integer not null primary key autoincrement, \"\n",
        "            \"term text unique not null, \"\n",
        "            \"document_frequency integer not null)\")\n",
        "\n",
        "        self.connection.execute(\"create table if not exists global_info (key text not null primary key, value integer)\")\n",
        "        # add global info default values if not present\n",
        "        self.connection.execute(\"insert or ignore into global_info values ('total_author_len', 0)\")\n",
        "        self.connection.execute(\"insert or ignore into global_info values ('total_title_len', 0)\")\n",
        "        self.connection.execute(\"insert or ignore into global_info values ('total_body_len', 0)\")\n",
        "        self.connection.execute(\"insert or ignore into global_info values ('num_docs', 0)\")\n",
        "\n",
        "        self.connection.execute(\"pragma optimize\")\n",
        "\n",
        "        self.connection.commit()\n",
        "        self.global_info_dirty = True\n",
        "        self.cached_global_info = None\n",
        "\n",
        "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\n",
        "            \"select doc_id, occurrences_author, occurrences_title, occurrences_body from postings where term_id = ? \"\n",
        "            \"order by doc_id\", (term_id,))\n",
        "        def row_factory(_cursor, row):\n",
        "            return Posting(row[0], term_id, {\"author\": row[1], \"title\": row[2], \"body\": row[3]})\n",
        "        cursor.row_factory = row_factory\n",
        "        yield from cursor\n",
        "\n",
        "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select author_len, title_len, body_len from document_info where doc_id = ?\", (doc_id,))\n",
        "        author_len, title_len, body_len = cursor.fetchone()\n",
        "        return DocumentInfo(doc_id, [author_len, title_len, body_len])\n",
        "\n",
        "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select author, title, body from document_contents where doc_id = ?\", (doc_id,))\n",
        "        author, title, body = cursor.fetchone()\n",
        "        return DocumentContents(author, title, body)\n",
        "\n",
        "    def get_term(self, term_id: int) -> Term:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select term, document_frequency from terms where term_id = ?\", (term_id,))\n",
        "        term, document_frequency = cursor.fetchone()\n",
        "        return Term(term, term_id, document_frequency=document_frequency)\n",
        "\n",
        "    def get_term_id(self, term: str) -> Optional[int]:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select term_id from terms where term = ?\", (term,))\n",
        "        result = cursor.fetchone()\n",
        "        return result[0] if result is not None else None\n",
        "\n",
        "    def get_global_info(self) -> dict[str, Any]:\n",
        "        if self.global_info_dirty:\n",
        "            cursor = self.connection.cursor()\n",
        "            cursor.execute(\"select key, value from global_info\")\n",
        "            global_info = cursor.fetchall()\n",
        "            global_info = {key: value for key, value in global_info}\n",
        "            self.cached_global_info = {\n",
        "                \"avg_field_lengths\": {\n",
        "                    \"author\": global_info[\"total_author_len\"] / global_info[\"num_docs\"],\n",
        "                    \"title\": global_info[\"total_title_len\"] / global_info[\"num_docs\"],\n",
        "                    \"body\": global_info[\"total_body_len\"] / global_info[\"num_docs\"]\n",
        "                },\n",
        "                \"num_docs\": global_info[\"num_docs\"]\n",
        "            }\n",
        "            self.global_info_dirty = False\n",
        "        return self.cached_global_info\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select value from global_info where key = 'num_docs'\")\n",
        "        return cursor.fetchone()[0]\n",
        "\n",
        "    def _increment_field_lengths(self, author_len: int, title_len: int, body_len: int) -> None:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"update global_info set value = value + ? where key = 'total_author_len'\", (author_len,))\n",
        "        cursor.execute(\"update global_info set value = value + ? where key = 'total_title_len'\", (title_len,))\n",
        "        cursor.execute(\"update global_info set value = value + ? where key = 'total_body_len'\", (body_len,))\n",
        "\n",
        "    def _create_or_get_term_id(self, term: str) -> int:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"insert or ignore into terms(term, document_frequency) values (?, 0)\", (term,))\n",
        "        cursor.execute(\"select term_id from terms where term = ?\", (term,))\n",
        "        return cursor.fetchone()[0]\n",
        "\n",
        "    def _new_document(self, doc: DocumentContents, author_len: int, title_len: int, body_len: int) -> int:\n",
        "        cursor = self.connection.cursor()\n",
        "        if doc.__dict__.get(\"doc_id\") is not None:\n",
        "            cursor.execute(\"insert into document_info(doc_id, author_len, title_len, body_len) values (?, ?, ?, ?)\", (doc.doc_id, author_len, title_len, body_len))\n",
        "        else:\n",
        "            cursor.execute(\"insert into document_info(author_len, title_len, body_len) values (?, ?, ?)\", (author_len, title_len, body_len))\n",
        "        doc_id = cursor.lastrowid\n",
        "        cursor.execute(\"insert into document_contents(doc_id, author, title, body) values (?, ?, ?, ?)\", (doc_id, doc.author, doc.title, doc.body))\n",
        "        cursor.execute(\"update global_info set value = value + 1 where key = 'num_docs'\")\n",
        "        return doc_id\n",
        "\n",
        "    def _update_postings(self, term_id: int, doc_id: int, location: TokenLocation) -> None:\n",
        "        increments = {\n",
        "            \"author\": 1 if location == TokenLocation.AUTHOR else 0,\n",
        "            \"title\": 1 if location == TokenLocation.TITLE else 0,\n",
        "            \"body\": 1 if location == TokenLocation.BODY else 0\n",
        "        }\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select occurrences_author, occurrences_title, occurrences_body from postings where term_id = ? and doc_id = ?\", (term_id, doc_id))\n",
        "        result = cursor.fetchone()\n",
        "        if result is None:\n",
        "            cursor.execute(\n",
        "                \"insert into postings(term_id, doc_id, occurrences_author, occurrences_title, occurrences_body) \"\n",
        "                \"values (?, ?, ?, ?, ?)\", (term_id, doc_id, increments[\"author\"], increments[\"title\"], increments[\"body\"]))\n",
        "        else:\n",
        "            cursor.execute(\n",
        "                \"update postings set occurrences_author = occurrences_author + ?, occurrences_title = occurrences_title + ?, \"\n",
        "                \"occurrences_body = occurrences_body + ? where term_id = ? and doc_id = ?\",\n",
        "                (increments[\"author\"], increments[\"title\"], increments[\"body\"], term_id, doc_id))\n",
        "\n",
        "    def _contains_document(self, doc_id: int) -> bool:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"select count(*) from document_info where doc_id = ?\", (doc_id,))\n",
        "        ret = cursor.fetchone()[0]\n",
        "        return ret > 0\n",
        "\n",
        "    def _increment_document_frequency(self, term_id: int) -> None:\n",
        "        cursor = self.connection.cursor()\n",
        "        cursor.execute(\"update terms set document_frequency = document_frequency + 1 where term_id = ?\", (term_id,))\n",
        "\n",
        "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
        "\n",
        "        if doc.__dict__.get(\"doc_id\") is not None:\n",
        "            if self._contains_document(doc.doc_id):\n",
        "                return\n",
        "        self.global_info_dirty = True\n",
        "\n",
        "        terms = tokenizer.tokenize_document(doc)\n",
        "        author_length = sum(1 for term in terms if term.location == TokenLocation.AUTHOR)\n",
        "        title_length = sum(1 for term in terms if term.location == TokenLocation.TITLE)\n",
        "        body_length = sum(1 for term in terms if term.location == TokenLocation.BODY)\n",
        "\n",
        "        self._increment_field_lengths(author_length, title_length, body_length)\n",
        "\n",
        "        encountered_terms = set()\n",
        "        term_ids_and_locations = []\n",
        "        for term in terms:\n",
        "            term_id = self._create_or_get_term_id(term.text)\n",
        "            if term_id not in encountered_terms:\n",
        "                self._increment_document_frequency(term_id)\n",
        "            encountered_terms.add(term_id)\n",
        "            term_ids_and_locations.append((term_id, term.location))\n",
        "        doc_id = self._new_document(doc, author_length, title_length, body_length)\n",
        "        for term_id, location in term_ids_and_locations:\n",
        "            self._update_postings(term_id, doc_id, location)\n",
        "        self.connection.commit()\n",
        "\n",
        "    def bulk_index_documents(self, docs, tokenizer, verbose = False):\n",
        "        super().bulk_index_documents(docs, tokenizer, verbose)\n",
        "        self.connection.execute(\"pragma optimize\")\n",
        "        self.connection.commit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": false,
        "id": "R1YHL7DzaW-n"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.priority_queue ===\n",
        "\n",
        "import heapq\n",
        "from typing import Iterable, Optional, Sized\n",
        "\n",
        "class PriorityQueue(Iterable[tuple[float, int]], Sized):\n",
        "    def __init__(self, max_size: int):\n",
        "        \"\"\"\n",
        "        Create a priority queue with a maximum size.\n",
        "        \"\"\"\n",
        "        self.heap = []\n",
        "        self.finalised = False\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def push(self, doc_id: int, score: float) -> Optional[int]:\n",
        "        \"\"\"\n",
        "        Add an item with a given score to the priority queue\n",
        "\n",
        "        Returns the doc_id of the item that was popped, if any. If the new item was not added returns its doc_id.\n",
        "        \"\"\"\n",
        "        if len(self) == self.max_size:\n",
        "            if score > self.heap[0][0]:\n",
        "                popped = heapq.heappushpop(self.heap, (score, doc_id))\n",
        "                return popped[1]\n",
        "            else:\n",
        "                return doc_id\n",
        "        else:\n",
        "            heapq.heappush(self.heap, (score, doc_id))\n",
        "            return None\n",
        "\n",
        "    def finalise(self):\n",
        "        \"\"\"\n",
        "        Call this after all items have been pushed to the priority queue.\n",
        "        \"\"\"\n",
        "        self.heap.sort(reverse=True)\n",
        "        self.finalised = True\n",
        "\n",
        "    def __iter__(self) -> Iterable[tuple[float, int]]:\n",
        "        \"\"\"\n",
        "        Iterate over the items in the priority queue.\n",
        "        \"\"\"\n",
        "        if not self.finalised:\n",
        "            raise ValueError(\"Priority queue must be finalised before iterating\")\n",
        "        return iter(self.heap)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of items in the priority queue.\n",
        "        \"\"\"\n",
        "        return len(self.heap)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhRUWKL1aW-n"
      },
      "source": [
        "# Ir system\n",
        "\n",
        "This class is the core of the project. All the components of the system are needed in order to construct an instance of this class.\n",
        "It uses the components to perform the indexing and search operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": false,
        "id": "obevKCzXaW-n"
      },
      "outputs": [],
      "source": [
        "#%% === mir.ir.ir ===\n",
        "\n",
        "from collections.abc import Generator\n",
        "# import string\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "# import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from more_itertools import peekable\n",
        "\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.ir.impls.default_index import DefaultIndex\n",
        "# from mir.ir.impls.count_scoring_function import CountScoringFunction\n",
        "# from mir.ir.impls.default_tokenizers import DefaultTokenizer\n",
        "# from mir.ir.index import Index\n",
        "# from mir.ir.priority_queue import PriorityQueue\n",
        "# from mir.ir.scoring_function import ScoringFunction\n",
        "# from mir.ir.tokenizer import Tokenizer\n",
        "# from mir.utils.sized_generator import SizedGenerator\n",
        "\n",
        "class Ir:\n",
        "    def __init__(self, index: Optional[Index] = None, tokenizer: Optional[Tokenizer] = None, scoring_functions: Optional[list[tuple[int, ScoringFunction]]] = None):\n",
        "        \"\"\"\n",
        "        Create an IR system.\n",
        "\n",
        "        # Parameters\n",
        "        - index (Index): The index to use. If None, a DefaultIndex is used.\n",
        "        - tokenizer (Tokenizer): The tokenizer to use. If None, a DefaultTokenizer is used.\n",
        "        - scoring_functions (Optional[list[tuple[int, ScoringFunction]]]): A list of scoring functions to use, with their respective top_k results to keep.\n",
        "        If None CountScoringFunction is used.\n",
        "        \"\"\"\n",
        "        self.index: Index = index if index is not None else DefaultIndex()\n",
        "        self.tokenizer: Tokenizer = tokenizer if tokenizer is not None else DefaultTokenizer()\n",
        "        self.scoring_functions: list[tuple[int, ScoringFunction]] = scoring_functions if scoring_functions is not None else [\n",
        "            (1000, CountScoringFunction())\n",
        "        ]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of documents in the index.\n",
        "        \"\"\"\n",
        "        return len(self.index)\n",
        "\n",
        "    def index_document(self, doc: DocumentContents) -> None:\n",
        "        \"\"\"\n",
        "        Index a document.\n",
        "\n",
        "        # Parameters\n",
        "        - doc (DocumentContents): The document to index.\n",
        "        \"\"\"\n",
        "        self.index.index_document(doc, self.tokenizer)\n",
        "\n",
        "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], verbose: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Bulk index documents.\n",
        "\n",
        "        # Parameters\n",
        "        - docs (SizedGenerator[DocumentContents, None, None]): A generator of documents to index.\n",
        "        - verbose (bool): Whether to show a progress bar.\n",
        "        \"\"\"\n",
        "        self.index.bulk_index_documents(docs, self.tokenizer, verbose)\n",
        "\n",
        "    def search(self, query: str) -> Generator[DocumentContents, None, None]:\n",
        "        \"\"\"\n",
        "        Search for documents based on a query.\n",
        "        Uses document-at-a-time scoring.\n",
        "\n",
        "        # Parameters\n",
        "        - query (str): The query to search for.\n",
        "        - scoring_functions (list[ScoringFunction]): A list of scoring functions to use.\n",
        "\n",
        "        # Yields\n",
        "        - DocumentContents: A document that matches the query. In decreasing order of score.\n",
        "        It also has a score attribute with the score of the document.\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(self.scoring_functions) > 0, \"At least one scoring function must be provided\"\n",
        "\n",
        "        ks, scoring_functions = zip(*self.scoring_functions)\n",
        "        scoring_functions: list[ScoringFunction] = list(scoring_functions)\n",
        "        ks: list[int] = list(ks)[::-1]\n",
        "\n",
        "        terms = self.tokenizer.tokenize_query(query)\n",
        "        term_ids = [term_id for term in terms if (\n",
        "            term_id := self.index.get_term_id(term.text)) is not None]\n",
        "        terms = [self.index.get_term(term_id) for term_id in term_ids]\n",
        "        posting_generators = [\n",
        "            peekable(self.index.get_postings(term_id)) for term_id in term_ids]\n",
        "\n",
        "        priority_queue = PriorityQueue(ks[-1])\n",
        "        first_scoring_function = scoring_functions[0]\n",
        "        postings_cache = {}\n",
        "\n",
        "        while True:\n",
        "            # find the lowest doc_id among all the posting lists\n",
        "            # doing this avoids having to iterate over all the doc_ids\n",
        "            # we only take into account the doc_ids that are present in the posting lists\n",
        "            lowest_doc_id = None\n",
        "            empty_posting_lists = []\n",
        "            for i, posting in enumerate(posting_generators):\n",
        "                try:\n",
        "                    doc_id = posting.peek().doc_id\n",
        "                    if lowest_doc_id is None or doc_id < lowest_doc_id:\n",
        "                        lowest_doc_id = doc_id\n",
        "                except StopIteration:\n",
        "                    empty_posting_lists.append(i)\n",
        "            # all the posting lists are empty\n",
        "            if lowest_doc_id is None:\n",
        "                break\n",
        "\n",
        "            # remove the empty posting lists\n",
        "            for i in reversed(empty_posting_lists):\n",
        "                posting_generators.pop(i)\n",
        "                term_ids.pop(i)\n",
        "\n",
        "            postings = []\n",
        "            # get all the postings with the current doc_id, and advance their iterators\n",
        "            for i, posting in enumerate(posting_generators):\n",
        "                if posting.peek().doc_id == lowest_doc_id:\n",
        "                    next_posting = next(posting)\n",
        "                    postings.append(next_posting)\n",
        "            postings_cache[lowest_doc_id] = postings\n",
        "            # now that we have all the info about the current document, we can score it\n",
        "            global_info = self.index.get_global_info()\n",
        "            document_info = self.index.get_document_info(lowest_doc_id)\n",
        "            score = first_scoring_function(document_info, postings, terms, **global_info)\n",
        "            # we add the score and doc_id to the priority queue\n",
        "            popped_doc_id = priority_queue.push(lowest_doc_id, score)\n",
        "            # if the priority queue is full, we remove the lowest score\n",
        "            if popped_doc_id is not None:\n",
        "                del postings_cache[popped_doc_id]\n",
        "\n",
        "        priority_queue.finalise()\n",
        "\n",
        "        for scoring_function in scoring_functions[1:]:\n",
        "            ks.pop()\n",
        "            resorted_documents = []\n",
        "            if scoring_function.batched_call is not None:\n",
        "                scores: list[float] = scoring_function.batched_call(\n",
        "                    [self.index.get_document_contents(doc_id).body for _, doc_id in priority_queue.heap[:ks[-1]]],\n",
        "                    query\n",
        "                )\n",
        "                for i, (score, doc_id) in enumerate(priority_queue.heap[:ks[-1]]):\n",
        "                    new_score = scores[i]\n",
        "                    resorted_documents.append((new_score + score, doc_id))\n",
        "            else:\n",
        "                for score, doc_id in priority_queue.heap[:ks[-1]]:\n",
        "                    postings = postings_cache[doc_id]\n",
        "                    global_info = self.index.get_global_info()\n",
        "                    global_info[\"document_content\"] = self.index.get_document_contents(doc_id).body\n",
        "                    global_info[\"query_content\"] = query\n",
        "                    new_score = scoring_function(self.index.get_document_info(doc_id), postings, terms, **global_info)\n",
        "                    # we add the old score to maintain monotonicity\n",
        "                    resorted_documents.append((new_score + score, doc_id))\n",
        "\n",
        "            resorted_documents.sort(key=lambda x: x[0], reverse=True)\n",
        "            priority_queue.heap = resorted_documents + priority_queue.heap[ks[-1]:]\n",
        "\n",
        "        for score, doc_id in priority_queue:\n",
        "            ret = self.index.get_document_contents(doc_id)\n",
        "            ret.add_field(\"id\", doc_id)\n",
        "            ret.set_score(score)\n",
        "            yield ret\n",
        "\n",
        "    def get_run(self, queries: pd.DataFrame, verbose: bool = False, pyterrier_compatible: bool = False) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generate a run file for the given queries in the form of a pandas DataFrame.\n",
        "        You can encode it to a file using a tab separator and the to_csv method.\n",
        "\n",
        "        # Parameters\n",
        "        - queries (pd.DataFrame): A DataFrame with the queries to run.\n",
        "        It must have the columns \"query_id\" and \"text\".\n",
        "        - verbose (bool): Whether to show a progress bar.\n",
        "\n",
        "        # Returns\n",
        "        - pd.DataFrame: The run file. It has the columns\n",
        "        \"query_id\", \"Q0\", \"document_no\", \"rank\", \"score\", \"run_id\".\n",
        "        If pyterrier_compatible is True, the columns are \"qid\", \"docid\", \"docno\", \"rank\", \"score\", \"query\".\n",
        "        \"\"\"\n",
        "\n",
        "        run = []\n",
        "        for _, query_row in tqdm(queries.iterrows(), desc=\"Running queries\", disable=not verbose, total=len(queries)):\n",
        "            query_id = query_row[\"query_id\"]\n",
        "            query = query_row[\"text\"]\n",
        "            for rank, doc in enumerate(self.search(query), start=0):\n",
        "                if pyterrier_compatible:\n",
        "                    run.append(\n",
        "                        {\"qid\": query_id, \"docid\": doc.id, \"docno\": doc.id, \"rank\": rank, \"score\": doc.score, \"query\": query})\n",
        "                else:\n",
        "                    run.append(\n",
        "                        {\"query_id\": query_id, \"Q0\": \"Q0\", \"doc_id\": doc.id, \"rank\": rank, \"score\": doc.score, \"run_id\": self.__class__.__name__})\n",
        "\n",
        "        run = pd.DataFrame(run)\n",
        "        return run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": false,
        "id": "R_F6FVISaW-o"
      },
      "outputs": [],
      "source": [
        "#%% === mir.utils.dataset ===\n",
        "\n",
        "from collections.abc import Generator\n",
        "import gzip\n",
        "import tarfile\n",
        "# import numpy as np\n",
        "# import requests\n",
        "# from mir import DATA_DIR, COLAB\n",
        "# import pandas as pd\n",
        "# import os\n",
        "import json\n",
        "# import unidecode\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from mir.ir.document_contents import DocumentContents\n",
        "# from mir.utils.sized_generator import SizedGenerator\n",
        "\n",
        "def get_msmarco_dataset(verbose: bool = False):\n",
        "    \"\"\"\n",
        "    Downloads the MS MARCO dataset to the data directory.\n",
        "    \"\"\"\n",
        "    corpus = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz\"\n",
        "    queries = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\"\n",
        "    queries_valid = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz\"\n",
        "    queries_test = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2020-queries.tsv.gz\"\n",
        "    qrels_train = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.train.tsv\"\n",
        "    # trec link is usually down so I'm using my own link to the same files\n",
        "    # qrels_valid = \"https://trec.nist.gov/data/deep/2019qrels-pass.txt\"\n",
        "    # qrels_test = \"https://trec.nist.gov/data/deep/2020qrels-pass.txt\"\n",
        "    qrels_valid = \"https://huggingface.co/Etto48/MIRProject/resolve/main/2019qrels-pass.txt\"\n",
        "    qrels_test = \"https://huggingface.co/Etto48/MIRProject/resolve/main/2020qrels-pass.txt\"\n",
        "\n",
        "    urls = [corpus, queries, queries_valid, queries_test, qrels_train, qrels_valid, qrels_test]\n",
        "    dataset_dir = f\"{DATA_DIR}/msmarco\"\n",
        "    os.makedirs(dataset_dir, exist_ok=True)\n",
        "    for url in urls:\n",
        "        file_name = url.split(\"/\")[-1]\n",
        "        path = f\"{dataset_dir}/{file_name}\"\n",
        "        if not os.path.exists(path):\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            file_size = int(response.headers.get(\"content-length\", 0))\n",
        "            block_size = 1024\n",
        "            try:\n",
        "                with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=f\"Downloading {file_name}\", disable=not verbose) as pbar:\n",
        "                    with open(path, \"wb\") as f:\n",
        "                        for data in response.iter_content(block_size):\n",
        "                            f.write(data)\n",
        "                            pbar.update(len(data))\n",
        "            except (KeyboardInterrupt, Exception) as e:\n",
        "                os.remove(path)\n",
        "                raise e\n",
        "        decompressed_path = path.replace(\".tar.gz\", \"\")\n",
        "        decompressed_path = decompressed_path.replace(\".gz\", \"\")\n",
        "        if file_name.endswith(\".tar.gz\") and not os.path.exists(f\"{decompressed_path}.tsv\"):\n",
        "            if verbose:\n",
        "                print(f\"Decompressing {file_name}...\")\n",
        "            with tarfile.open(path, \"r:gz\") as tar:\n",
        "                tar.extractall(dataset_dir, filter=\"fully_trusted\")\n",
        "        elif not file_name.endswith(\".tar.gz\") and \\\n",
        "                file_name.endswith(\".gz\") and \\\n",
        "                not os.path.exists(decompressed_path):\n",
        "            if verbose:\n",
        "                print(f\"Decompressing {file_name}...\")\n",
        "            with gzip.open(path, \"rb\") as f_in:\n",
        "                with open(decompressed_path, \"wb\") as f_out:\n",
        "                    f_out.write(f_in.read())\n",
        "\n",
        "def msmarco_dataset_to_contents(corpus: pd.DataFrame, verbose: bool = False) -> SizedGenerator[DocumentContents, None, None]:\n",
        "    \"\"\"\n",
        "    Returns the number of documents and a generator of DocumentContents from the test corpus.\n",
        "    \"\"\"\n",
        "    def inner() -> Generator[DocumentContents, None, None]:\n",
        "        for _, row in corpus.iterrows():\n",
        "            yield DocumentContents(author=\"\", title=\"\", body=row['text'], doc_id=int(row['docno']))\n",
        "    return SizedGenerator(inner(), len(corpus))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": false,
        "id": "Z9G7x-P3aW-o"
      },
      "outputs": [],
      "source": [
        "#%% === mir.utils.download_and_extract ===\n",
        "\n",
        "# import os\n",
        "# import tarfile\n",
        "# import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def download_and_extract(url: str, path: str, desc: str = \"\"):\n",
        "    stream = requests.get(url, stream=True)\n",
        "    total_size = int(stream.headers.get('content-length', 0))\n",
        "    tgz_path = f\"{path}.tar.gz\"\n",
        "    output_dir = f\"{path}\"\n",
        "    if not os.path.exists(tgz_path):\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024, desc=f\"Downloading {desc}\") as pbar:\n",
        "            with open(tgz_path, 'wb') as f:\n",
        "                for chunk in stream.iter_content(chunk_size=1024):\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "    if not os.path.exists(output_dir):\n",
        "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
        "            members = tqdm(tar.getmembers(), desc=f\"Extracting {desc}\")\n",
        "            tar.extractall(output_dir, members)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XcMuLmeaW-o"
      },
      "source": [
        "# Demo\n",
        "\n",
        "Now we will use the system to index ms-marco and run the test queries.\n",
        "Then we will compare the results with the ones of PyTerrier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": false,
        "id": "WDB-pYcxaW-p",
        "outputId": "fb288209-372b-4c56-8aa6-589351e1e3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738,
          "referenced_widgets": [
            "87dd5f993c0942d6a6ce895a71763834",
            "6a56752a23cd41fbbd375db3c0bf7a2c",
            "d8e29aac47444d84afb99010eb7370a7",
            "27293c7119064c58a519df7353212fda",
            "6267987cbf344633b76e33cc16cd1fa5",
            "5ed5541679454d668bc1e05c988a09fb",
            "58bb27e739b7470f9927c892904e3d98",
            "85630e88d43740039ba2ed3a9a127ad8",
            "3f53e51f7a30479ca437123cbfdb394a",
            "99db674ce4fb440288bc3cf8b14ff7fa",
            "dc9608f01e45477c9ab0d29928ac1d54"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decompressing queries.tar.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/contriever-msmarco were not used when initializing Contriever: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing Contriever from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Contriever from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running queries:   0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87dd5f993c0942d6a6ce895a71763834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([0])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "a Tensor with 0 elements cannot be converted to Scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/content/colab.ipynb\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mmy_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmy_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_ir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyterrier_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mbm25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterrier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BM25\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colab.ipynb\u001b[0m in \u001b[0;36mget_run\u001b[0;34m(self, queries, verbose, pyterrier_compatible)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mquery_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpyterrier_compatible\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     run.append(\n",
            "\u001b[0;32m/content/colab.ipynb\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mglobal_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"document_content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mglobal_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mnew_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_document_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mglobal_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                     \u001b[0;31m# we add the old score to maintain monotonicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mresorted_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/colab.ipynb\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, document, postings, query, document_content, query_content, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_queries_and_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_content\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocument_content\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatched_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_contents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_contents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 0 elements cannot be converted to Scalar"
          ]
        }
      ],
      "source": [
        "#%% === mir.scripts.demo ===\n",
        "\n",
        "# import os\n",
        "import re\n",
        "import pyterrier as pt\n",
        "from pyterrier import IndexFactory\n",
        "# import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from mir import DATA_DIR\n",
        "# from mir.ir.impls.bm25f_scoring import BM25FScoringFunction\n",
        "# from mir.ir.impls.neural_scoring_function import NeuralScoringFunction\n",
        "# from mir.ir.impls.sqlite_index import SqliteIndex\n",
        "# from mir.ir.ir import Ir\n",
        "# from mir.utils.dataset import get_msmarco_dataset, msmarco_dataset_to_contents\n",
        "# from mir.utils.download_and_extract import download_and_extract\n",
        "\n",
        "\n",
        "get_msmarco_dataset(verbose=True)\n",
        "dataset_csv = f\"{DATA_DIR}/msmarco/collection.tsv\"\n",
        "index_path = f\"{DATA_DIR}/msmarco-pyterrier-index/data.properties\"\n",
        "msmarco_pyterrier_index_url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/msmarco-pyterrier-index.tar.gz\"\n",
        "msmarco_sqlite_index_url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/msmarco-sqlite-index.db.tar.gz\"\n",
        "# download pyterrier index\n",
        "download_and_extract(msmarco_pyterrier_index_url, DATA_DIR, desc=\"PyTerrier Index\")\n",
        "# download sqlite index\n",
        "download_and_extract(msmarco_sqlite_index_url, DATA_DIR, desc=\"SQLite Index\")\n",
        "\n",
        "indexer = pt.terrier.IterDictIndexer(f\"{DATA_DIR}/msmarco-pyterrier-index\")\n",
        "if not os.path.exists(index_path):\n",
        "    dataset = pd.read_csv(dataset_csv, sep='\\t', header=None, names=['docno', 'text'], dtype={'docno': str, 'text': str})\n",
        "    indexref = indexer.index(tqdm(dataset.to_dict(orient='records'), desc=\"Indexing\"))\n",
        "    del dataset\n",
        "else:\n",
        "    indexref = pt.IndexRef.of(index_path)\n",
        "index = IndexFactory.of(indexref)\n",
        "\n",
        "\n",
        "topics_path = f\"{DATA_DIR}/msmarco/msmarco-test2020-queries.tsv\"\n",
        "qrels_path = f\"{DATA_DIR}/msmarco/2020qrels-pass.txt\"\n",
        "\n",
        "topics = pd.read_csv(topics_path, sep='\\t', header=None, names=['qid', 'query'], dtype={'qid': str, 'query': str})\n",
        "qrels = pd.read_csv(qrels_path, sep=' ', header=None, names=['qid', 'Q0', 'docno', 'relevance'], dtype={'qid': str, 'Q0': str, 'docno': str, 'relevance': int})\n",
        "\n",
        "def preprocess_query(query):\n",
        "    query = re.sub(r'[^\\w\\s]', '', query)\n",
        "    query = query.lower()\n",
        "    return query\n",
        "\n",
        "topics['query'] = topics['query'].apply(preprocess_query)\n",
        "\n",
        "my_ir = Ir(SqliteIndex(f\"{DATA_DIR}/msmarco-sqlite-index.db\"), scoring_functions=[\n",
        "    (100, BM25FScoringFunction(1.2, 0.8)),\n",
        "    (10, NeuralScoringFunction())\n",
        "])\n",
        "if len(my_ir.index) == 0:\n",
        "    dataset = pd.read_csv(dataset_csv, sep='\\t', header=None, names=['docno', 'text'], dtype={'docno': str, 'text': str})\n",
        "    sized_generator = msmarco_dataset_to_contents(dataset)\n",
        "    my_ir.bulk_index_documents(sized_generator, verbose=True)\n",
        "\n",
        "my_topics = pd.read_csv(topics_path, sep='\\t', header=None, names=['query_id', 'text'], dtype={'query_id': int, 'text': str})\n",
        "my_run = my_ir.get_run(my_topics, verbose=True, pyterrier_compatible=True)\n",
        "\n",
        "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n",
        "dfree = pt.terrier.Retriever(index, wmodel=\"DFRee\")\n",
        "pyterrier_models = {\n",
        "    \"BM25\": bm25 % 100,\n",
        "    \"BM25+DFRee\": (bm25 % 100) >> dfree\n",
        "}\n",
        "pyterrier_runs = {}\n",
        "for model_name, model in pyterrier_models.items():\n",
        "    print(f\"Running PyTerrier {model_name}\")\n",
        "    pyterrier_runs[model_name] = model.transform(topics)\n",
        "\n",
        "test_runs = [my_run, *pyterrier_runs.values()]\n",
        "names = [\"MyIR\", *pyterrier_models.keys()]\n",
        "\n",
        "metrics = [\"map\", \"ndcg\", \"recip_rank\", \"P.10\", \"recall.10\", ]\n",
        "res = pt.Experiment(test_runs, topics, qrels, metrics, names=names)\n",
        "print(res)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87dd5f993c0942d6a6ce895a71763834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a56752a23cd41fbbd375db3c0bf7a2c",
              "IPY_MODEL_d8e29aac47444d84afb99010eb7370a7",
              "IPY_MODEL_27293c7119064c58a519df7353212fda"
            ],
            "layout": "IPY_MODEL_6267987cbf344633b76e33cc16cd1fa5"
          }
        },
        "6a56752a23cd41fbbd375db3c0bf7a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed5541679454d668bc1e05c988a09fb",
            "placeholder": "",
            "style": "IPY_MODEL_58bb27e739b7470f9927c892904e3d98",
            "value": "Runningqueries:0%"
          }
        },
        "d8e29aac47444d84afb99010eb7370a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85630e88d43740039ba2ed3a9a127ad8",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f53e51f7a30479ca437123cbfdb394a",
            "value": 1
          }
        },
        "27293c7119064c58a519df7353212fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99db674ce4fb440288bc3cf8b14ff7fa",
            "placeholder": "",
            "style": "IPY_MODEL_dc9608f01e45477c9ab0d29928ac1d54",
            "value": "1/200[00:01&lt;06:23,1.93s/it]"
          }
        },
        "6267987cbf344633b76e33cc16cd1fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed5541679454d668bc1e05c988a09fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58bb27e739b7470f9927c892904e3d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85630e88d43740039ba2ed3a9a127ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f53e51f7a30479ca437123cbfdb394a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99db674ce4fb440288bc3cf8b14ff7fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9608f01e45477c9ab0d29928ac1d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}