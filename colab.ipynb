{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "version": "3.12"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook was automatically generated with our custom script (mir.scripts.ipynb_compiler)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multimedia Information Retrieval - Project\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Etto48/MIRProject/blob/main/colab.ipynb)\n",
                "\n",
                "This project was developed by \"The Karate Kid\" team:\n",
                "\n",
                "- [Ettore Ricci](https://github.com/Etto48)\n",
                "- [Paolo Palumbo](https://github.com/paolpal)\n",
                "- [Erni Deliallisi](https://github.com/erni-de)\n",
                "\n",
                "The whole codebase can be found on [GitHub](https://github.com/Etto48/MIRProject).\n",
                "\n",
                "## Introduction\n",
                "\n",
                "In this project, we aimed to develop an Information Retrieval (IR) system by leveraging both well-established techniques, such as BM25, and more advanced approaches, including a learning-to-rank model powered by a BERT-based language model.\n",
                "\n",
                "We used the [MSMarco Dataset](https://microsoft.github.io/msmarco/).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "# install dependencies\n",
                "%pip install pandas tqdm iprogress ipywidgets unidecode nltk more_itertools python-terrier torch transformers psutil\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "# define __file__ and set env variable\n",
                "import os\n",
                "__file__ = os.path.abspath('colab.ipynb')\n",
                "os.environ['MIR_NOTEBOOK'] = __file__\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir ===\n",
                "\n",
                "import os\n",
                "import pandas as pd\n",
                "pd.options.mode.copy_on_write = True\n",
                "\n",
                "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
                "    COLAB = True\n",
                "else:\n",
                "    COLAB = False\n",
                "    \n",
                "if COLAB or os.getenv(\"MIR_NOTEBOOK\") is not None:\n",
                "    PROJECT_DIR = os.path.abspath(\"./\")\n",
                "else:\n",
                "    PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
                "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
                "\n",
                "if not os.path.exists(DATA_DIR):\n",
                "    os.mkdir(DATA_DIR)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Document Contents\n",
                "\n",
                "The `DocumentContents` class represents a document in a format compatible with the system. \n",
                "\n",
                "It includes three primary fields: author, title, and body.\n",
                " \n",
                "Additionally, it supports dynamic expansion with optional fields using keyword arguments (kwargs).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.document_contents ===\n",
                "\n",
                "\n",
                "\n",
                "class DocumentContents:\n",
                "    def __init__(self, author: str, title: str, body: str, **kwargs):\n",
                "        self.author = author\n",
                "        self.title = title\n",
                "        self.body = body\n",
                "        self.__dict__.update(kwargs)\n",
                "        \n",
                "    def add_field(self, field: str, value: str):\n",
                "        self.__dict__[field] = value\n",
                "        \n",
                "    def set_score(self, score: float):\n",
                "        self.score = score\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Tokens\n",
                "Now we defines structures to manage and classify tokens within a document or query.\n",
                "\n",
                "- `TokenLocation`: An enumeration (Enum) that specifies the possible locations of a token, including QUERY, AUTHOR, TITLE, and BODY.\n",
                "\n",
                "- `Token`: A data class (@dataclass) representing a token with two attributes:\n",
                "\n",
                "\t- `text`: The token's string value.\n",
                "\t- `location`: A TokenLocation enum instance that indicates where the token is located (e.g., in the query, author field, title, or body).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.token_ir ===\n",
                "\n",
                "from dataclasses import dataclass\n",
                "from enum import Enum\n",
                "\n",
                "\n",
                "class TokenLocation(Enum) :\n",
                "    QUERY = 0\n",
                "    AUTHOR = 1\n",
                "    TITLE = 2\n",
                "    BODY = 3\n",
                "\n",
                "@dataclass\n",
                "class Token:\n",
                "    text: str\n",
                "    location: TokenLocation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenizer\n",
                "\n",
                "This is an abstract class that defines the interface tokenizer implementations must expose to ensure compatibility with the project.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.tokenizer ===\n",
                "\n",
                "from abc import abstractmethod\n",
                "from typing import Protocol\n",
                "\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.token_ir import Token\n",
                "\n",
                "\n",
                "class Tokenizer(Protocol):\n",
                "    @abstractmethod\n",
                "    def tokenize_query(self, query: str) -> list[Token]:\n",
                "        \"\"\"\n",
                "        Tokenize a query.\n",
                "\n",
                "        # Parameters\n",
                "        - query (str): The query to tokenize.\n",
                "\n",
                "        # Returns\n",
                "        - list[Token]: The tokens of the query.\n",
                "        \"\"\"\n",
                "    @abstractmethod\n",
                "    def tokenize_document(self, doc: DocumentContents) -> list[Token]:\n",
                "        \"\"\"\n",
                "        Tokenize a document.\n",
                "\n",
                "        # Parameters\n",
                "        - doc (DocumentContents): The document to tokenize.\n",
                "\n",
                "        # Returns\n",
                "        - list[Token]: The tokens of the document.\n",
                "        \"\"\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Document Info\n",
                "\n",
                "The `DocumentInfo` class stores a document's ID and token counts for author, title, and body. \n",
                "\n",
                "The `from_document_contents` method generates an instance by tokenizing a document and counting tokens per field.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.document_info ===\n",
                "\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.token_ir import TokenLocation\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "\n",
                "\n",
                "class DocumentInfo:\n",
                "    def __init__(self, id: int, lengths: list[int]):\n",
                "        assert len(lengths) == 3, \"Lengths must have 3 elements, [author, title, body]\"\n",
                "        self.id = id\n",
                "        self.lengths = lengths\n",
                "\n",
                "    @staticmethod\n",
                "    def from_document_contents(id: int, doc: DocumentContents, tokenizer: Tokenizer) -> \"DocumentInfo\":\n",
                "        tokens = tokenizer.tokenize_document(doc)\n",
                "        tokens_for_field = [0,0,0]\n",
                "        for token in tokens:\n",
                "            match token.location:\n",
                "                case TokenLocation.AUTHOR:\n",
                "                    field_offset = 0\n",
                "                case TokenLocation.TITLE:\n",
                "                    field_offset = 1\n",
                "                case TokenLocation.BODY:\n",
                "                    field_offset = 2\n",
                "                case _:\n",
                "                    raise ValueError(f\"Invalid token location {token.location}\")\n",
                "            tokens_for_field[field_offset] += 1\n",
                "        return DocumentInfo(id, tokens_for_field)        \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Posting\n",
                "\n",
                "The `Posting` class represents a term's occurrence in a document, storing its `doc_id`, `term_id`, and a dictionary of term frequencies across author, title, and body fields. \n",
                "\n",
                "If no frequencies are provided, it defaults to 0 for each field. The `__repr__` method returns a string representation of the posting.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.posting ===\n",
                "\n",
                "from typing import Optional\n",
                "\n",
                "class Posting:\n",
                "    def __init__(self, doc_id: int, term_id: int, occurrences: Optional[dict[str, int]] = None):\n",
                "        self.term_id = term_id\n",
                "        self.doc_id = doc_id\n",
                "        self.occurrences = occurrences if occurrences is not None else {\"author\": 0, \"title\": 0, \"body\": 0}\n",
                "\n",
                "    def __repr__(self) -> str:\n",
                "        return f\"Posting(doc_id={self.doc_id}, term_id={self.term_id}, occurrences={self.occurrences})\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Term\n",
                "\n",
                "The `Term` class represents a term in the lexicon, storing the term's string value, its unique id, and its document frequency (DF) as the main piece of additional information, along with any other data via keyword arguments (kwargs). \n",
                "\n",
                "These elements collectively form the lexicon for the system.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.term ===\n",
                "\n",
                "class Term:\n",
                "    def __init__(self, term: str, id: int, **kwargs):\n",
                "        self.term = term\n",
                "        self.id = id\n",
                "        self.info = kwargs\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Scoring Function\n",
                "\n",
                "The `ScoringFunction` class defines the interface for scoring documents based on postings and the query, supporting reranking in the system. \n",
                "\n",
                "It includes a `__call__` method for scoring and an optional `batched_call()` function for processing multiple documents at once, mainly for neural networks' efficiency.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.scoring_function ===\n",
                "\n",
                "from typing import Any, Callable, Optional, Protocol\n",
                "\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.term import Term\n",
                "\n",
                "\n",
                "class ScoringFunction(Protocol):\n",
                "    batched_call: Optional[Callable[[\"ScoringFunction\",list[str],str], list[float]]] = None\n",
                "    def __call__(self, document_info: DocumentInfo, postings: list[Posting], query: list[Term], **kwargs: dict[str, Any]) -> float:\n",
                "        \"\"\"\n",
                "        Score a document based on the postings and the query.\n",
                "\n",
                "        # Parameters\n",
                "        - document_info (DocumentInfo): The document info relative to the document to score.\n",
                "        - postings (list[Posting]): The postings related to the document and the query.\n",
                "        - query (list[Term]): The query terms.\n",
                "        - **kwargs (dict[str, Any]): Additional arguments for the scoring function.\n",
                "\n",
                "        # Returns\n",
                "        - float: The score of the document.\n",
                "        \"\"\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## BM25F\n",
                "\n",
                "The `BM25FScoringFunction` class implements the BM25F scoring function, which is used for document ranking based on the frequency of query terms and their distribution across different fields like `title`, `body`, and `author`. It uses parameters such as `k1` and `b` to adjust term frequency and field length normalization. \n",
                "\n",
                "The class includes:\n",
                "- A method to build a dictionary of postings for quick lookup.\n",
                "- A `__call__` method to calculate the document score by iterating over the query terms.\n",
                "- A helper method `_rsv` to compute the **relevance score** based on term frequency and inverse document frequency (IDF).\n",
                "- A method `_wtf` to calculate **term frequency** with field-specific **weight** adjustments and length normalization.\n",
                "\n",
                "This structure enables efficient BM25F-based scoring for document retrieval.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.bm25f_scoring ===\n",
                "\n",
                "from typing import List, Dict\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.scoring_function import ScoringFunction\n",
                "# from mir.ir.term import Term\n",
                "import math\n",
                "\n",
                "\n",
                "class BM25FScoringFunction(ScoringFunction):\n",
                "    def __init__(self, k1: float = 1.5, b: float = 0.75, field_weights: Dict[str, float] = None):\n",
                "        self.k1 = k1\n",
                "        self.b = b\n",
                "        self.field_weights = field_weights if field_weights is not None else {'title': 2.0, 'body': 1.0, 'author': 0.5}\n",
                "\n",
                "    def _build_postings_dict(self, postings: List[Posting]) -> Dict[int, Posting]:\n",
                "        return {posting.term_id: posting for posting in postings}\n",
                "\n",
                "    def __call__(self, document: DocumentInfo, postings: List[Posting], query: List[Term], *, num_docs: int, avg_field_lengths: dict[str, int], **_) -> float:\n",
                "        postings_dict = self._build_postings_dict(postings)\n",
                "        score = 0.0\n",
                "        for term in query:\n",
                "            if term.id in postings_dict:\n",
                "                score += self._rsv(term, document, num_docs, postings_dict, avg_field_lengths)\n",
                "        return score\n",
                "\n",
                "    def _rsv(self, term: Term, document: DocumentInfo, num_docs: int, postings_dict: dict[int, Posting], avg_field_lengths: dict[str, int]) -> float:\n",
                "        tfd = self._wtf(term, document, postings_dict, avg_field_lengths)\n",
                "        idf = math.log(num_docs / term.info['document_frequency'])\n",
                "\n",
                "        if tfd > 0:\n",
                "            return (tfd / (self.k1 + tfd)) * idf\n",
                "        return 0.0\n",
                "\n",
                "    def _wtf(self, term: Term, document: DocumentInfo, postings_dict: dict[int, Posting], avg_field_lengths: dict[str, int]) -> float:\n",
                "        tfd = 0.0\n",
                "        field_indices = {\"author\": 0, \"title\": 1, \"body\": 2}\n",
                "\n",
                "        if term.id not in postings_dict:\n",
                "            return 0.0\n",
                "\n",
                "        posting = postings_dict[term.id]\n",
                "        for field, weight in self.field_weights.items():\n",
                "            tf = posting.occurrences.get(field, 0)\n",
                "            if tf == 0:\n",
                "                continue\n",
                "            field_index = field_indices[field]\n",
                "            avg_dlf = avg_field_lengths[field]\n",
                "            bb = 1 - self.b + self.b * document.lengths[field_index] / avg_dlf\n",
                "            tfd += weight * tf / bb\n",
                "\n",
                "        return tfd\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Count Scoring\n",
                "\n",
                "\n",
                "The `CountScoringFunction` class implements a simple scoring function that calculates the score of a document based on the ratio of the number of terms matching the query to the number of query terms.\n",
                "\n",
                "It returns this ratio as the document's score. \n",
                "\n",
                "This scoring function is basic and doesn't take into account term frequency or document length.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.count_scoring_function ===\n",
                "\n",
                "# from mir.ir.scoring_function import ScoringFunction\n",
                "\n",
                "\n",
                "class CountScoringFunction(ScoringFunction):\n",
                "    def __call__(self, document, postings, query, **kwargs):\n",
                "        return len(postings) / len(query)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.utils.sized_generator ===\n",
                "\n",
                "from collections.abc import Generator, Sized\n",
                "from typing import TypeVar, Generic\n",
                "\n",
                "T = TypeVar('T')\n",
                "P = TypeVar('P')\n",
                "Q = TypeVar('Q')\n",
                "\n",
                "\n",
                "class SizedGenerator(Generic[T, P, Q], Generator[T, P, Q], Sized):\n",
                "    def __init__(self, generator: Generator[T, P, Q], length: int):\n",
                "        self.generator = generator\n",
                "        self.length = length\n",
                "\n",
                "    def __iter__(self):\n",
                "        return self.generator\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.length\n",
                "\n",
                "    def send(self, value):\n",
                "        return self.generator.send(value)\n",
                "\n",
                "    def throw(self, typ, val=None, tb=None):\n",
                "        return self.generator.throw(typ, val, tb)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Index\n",
                "\n",
                "The `Index` class defines the interface for document indexing systems, holding the inverted index and providing methods for interacting with it. \n",
                "\n",
                "It includes functions to retrieve document and term information, index individual and bulk documents, and access global statistics. Key methods include `get_postings()` for term postings, `get_document_info()` and `get_document_contents()` for document metadata and content, and `get_term()` and `get_term_id()` for term details. \n",
                "\n",
                "This class serves as a foundation for implementing specific index types in search systems.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.index ===\n",
                "\n",
                "from abc import abstractmethod\n",
                "from collections.abc import Generator\n",
                "from typing import Any, Optional, Protocol\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.term import Term\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "# from mir.utils.sized_generator import SizedGenerator\n",
                "\n",
                "\n",
                "class Index(Protocol):\n",
                "    def get_global_info(self) -> dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Get global info from the index.\n",
                "\n",
                "        # Returns\n",
                "        - dict[str, int]: A dictionary with global info.\n",
                "        \"\"\"\n",
                "        return {}\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
                "        \"\"\"\n",
                "        Get a generator of postings for a term_id.\n",
                "        MUST be sorted by doc_id.\n",
                "\n",
                "        # Parameters\n",
                "        - term_id (int): The term_id.\n",
                "\n",
                "        # Yields\n",
                "        - Posting: A posting from the posting list related to the term_id.\n",
                "        \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
                "        \"\"\"\n",
                "        Get document info from a doc_id.\n",
                "\n",
                "        # Parameters\n",
                "        - doc_id (int): The doc_id.\n",
                "\n",
                "        # Returns\n",
                "        - DocumentInfo: The document info related to the doc_id.\n",
                "        \"\"\"\n",
                "    \n",
                "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
                "        \"\"\"\n",
                "        Get document contents from a doc_id.\n",
                "\n",
                "        # Parameters\n",
                "        - doc_id (int): The doc_id.\n",
                "\n",
                "        # Returns\n",
                "        - DocumentContents: The document contents related to the doc_id.\n",
                "        \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_term(self, term_id: int) -> Term:\n",
                "        \"\"\"\n",
                "        Get term info from a term_id.\n",
                "\n",
                "        # Parameters\n",
                "        - term_id (int): The term_id.\n",
                "\n",
                "        # Returns\n",
                "        - Term: The term related to the term_id.\n",
                "        \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def get_term_id(self, term: str) -> Optional[int]:\n",
                "        \"\"\"\n",
                "        Get term_id from a term in string format.\n",
                "        Returns None if the term is not in the index.\n",
                "\n",
                "        # Parameters\n",
                "        - term (str): The term in string format.\n",
                "\n",
                "        # Returns\n",
                "        - Optional[int]: The term_id related to the term or None if the term is not in the index.\n",
                "        \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def __len__(self) -> int:\n",
                "        \"\"\"\n",
                "        Get the number of documents in the index.\n",
                "\n",
                "        # Returns\n",
                "        - int: The number of documents in the index.\n",
                "        \"\"\"\n",
                "\n",
                "    @abstractmethod\n",
                "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
                "        \"\"\"\n",
                "        Add a document to the index.\n",
                "\n",
                "        # Parameters\n",
                "        - doc (DocumentContents): The document to add to the index.\n",
                "        - tokenizer (Tokenizer): The tokenizer to use to tokenize the document.\n",
                "        \"\"\"\n",
                "\n",
                "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], tokenizer: Tokenizer, verbose: bool = False) -> None:\n",
                "        \"\"\"\n",
                "        Add multiple documents to the index, this calls index_document for each document.\n",
                "\n",
                "        # Parameters\n",
                "        - docs (SizedGenerator[DocumentContents, None, None]): A generator of documents to add to the index.\n",
                "        - tokenizer (Tokenizer): The tokenizer to use to tokenize the documents.\n",
                "        - verbose (bool): Whether to show a progress bar.\n",
                "        \"\"\"        \n",
                "        for doc in tqdm(docs, desc=\"Indexing documents\", disable=not verbose, total=len(docs)):\n",
                "            self.index_document(doc, tokenizer)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Default Index\n",
                "\n",
                "The `DefaultIndex` class implements an index for storing and retrieving document and term information. \n",
                "\n",
                "It supports document indexing, term frequency tracking, and postings retrieval. \n",
                "\n",
                "It also allows bulk indexing for efficiency and provides persistence through saving and loading with pickle. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.default_index ===\n",
                "\n",
                "from collections import OrderedDict\n",
                "from collections.abc import Generator\n",
                "# import os\n",
                "import pickle\n",
                "from typing import Any, Optional\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.index import Index\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.term import Term\n",
                "# from mir.ir.token_ir import TokenLocation\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "# from mir.utils.sized_generator import SizedGenerator\n",
                "\n",
                "\n",
                "class DefaultIndex(Index):\n",
                "    def __init__(self, path: Optional[str] = None):\n",
                "        super().__init__()\n",
                "        self.postings: list[OrderedDict[Posting]] = []\n",
                "        self.document_info: list[DocumentInfo] = []\n",
                "        self.document_contents: list[DocumentContents] = []\n",
                "        self.terms: list[Term] = []\n",
                "        self.term_lookup: dict[str, int] = {}\n",
                "        self.path = None\n",
                "        self.total_field_lengths = {\n",
                "            \"author\": 0,\n",
                "            \"title\": 0,\n",
                "            \"body\": 0\n",
                "        }\n",
                "        if path is not None:\n",
                "            self.path = path\n",
                "            if os.path.exists(path):\n",
                "                self.load()\n",
                "    \n",
                "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
                "        for doc_id, posting in self.postings[term_id].items():\n",
                "            yield posting\n",
                "\n",
                "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
                "        return self.document_info[doc_id]\n",
                "    \n",
                "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
                "        return self.document_contents[doc_id]\n",
                "\n",
                "    def get_term(self, term_id: int) -> Term:\n",
                "        return self.terms[term_id]\n",
                "\n",
                "    def get_term_id(self, term: str) -> Optional[int]:\n",
                "        return self.term_lookup.get(term)\n",
                "    \n",
                "    def get_global_info(self) -> dict[str, Any]:\n",
                "        return {\n",
                "            \"avg_field_lengths\": {\n",
                "                \"author\": self.total_field_lengths[\"author\"] / len(self.document_info),\n",
                "                \"title\": self.total_field_lengths[\"title\"] / len(self.document_info),\n",
                "                \"body\": self.total_field_lengths[\"body\"] / len(self.document_info)\n",
                "            },\n",
                "            \"num_docs\": len(self.document_info)\n",
                "        }\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.document_info)\n",
                "\n",
                "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
                "        terms = tokenizer.tokenize_document(doc)\n",
                "        author_length = sum(1 for term in terms if term.location == TokenLocation.AUTHOR)\n",
                "        title_length = sum(1 for term in terms if term.location == TokenLocation.TITLE)\n",
                "        body_length = sum(1 for term in terms if term.location == TokenLocation.BODY)\n",
                "        self.total_field_lengths[\"author\"] += author_length\n",
                "        self.total_field_lengths[\"title\"] += title_length\n",
                "        self.total_field_lengths[\"body\"] += body_length\n",
                "        term_ids = []\n",
                "        for term in terms:\n",
                "            if term.text not in self.term_lookup:\n",
                "                term_id = len(self.terms)\n",
                "                self.terms.append(Term(term.text, term_id))\n",
                "                self.term_lookup[term.text] = term_id\n",
                "            else:\n",
                "                term_id = self.term_lookup[term.text]\n",
                "            term_ids.append(term_id)\n",
                "        doc_id = len(self.document_info)\n",
                "        self.document_info.append(DocumentInfo.from_document_contents(doc_id, doc, tokenizer))\n",
                "        self.document_contents.append(doc)\n",
                "        for term_id in term_ids:\n",
                "            if term_id >= len(self.postings):\n",
                "                self.postings.append(OrderedDict())\n",
                "            self.postings[term_id][doc_id] = Posting(doc_id, term_id)\n",
                "\n",
                "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], tokenizer: Tokenizer, verbose: bool = False) -> None:\n",
                "        super().bulk_index_documents(docs, tokenizer, verbose)\n",
                "        if self.path is not None:\n",
                "            self.save()\n",
                "\n",
                "    def load(self):\n",
                "        if self.path is not None:\n",
                "            try:\n",
                "                with open(self.path, \"rb\") as f:\n",
                "                    postings, document_info, document_contents, terms, term_lookup = pickle.load(f)\n",
                "                assert isinstance(postings, list)\n",
                "                assert isinstance(document_info, list)\n",
                "                assert isinstance(document_contents, list)\n",
                "                assert isinstance(terms, list)\n",
                "                assert isinstance(term_lookup, dict)\n",
                "            except Exception as e:\n",
                "                pass\n",
                "            else:\n",
                "                self.postings = postings\n",
                "                self.document_info = document_info\n",
                "                self.document_contents = document_contents\n",
                "                self.terms = terms\n",
                "                self.term_lookup = term_lookup\n",
                "        else:\n",
                "            raise ValueError(\"Path not set for index.\")\n",
                "\n",
                "    def save(self):\n",
                "        if self.path is not None:\n",
                "            with open(self.path, \"wb\") as f:\n",
                "                pickle.dump((self.postings, self.document_info, self.document_contents, self.terms, self.term_lookup), f)\n",
                "        else:\n",
                "            raise ValueError(\"Path not set for index.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Default Tokenizer\n",
                "\n",
                "The `DefaultTokenizer` class tokenizes and preprocesses text for information retrieval tasks.\n",
                "\n",
                "It uses NLTK's SnowballStemmer, removes punctuation, separates numbers, and eliminates stopwords.\n",
                "\n",
                "It provides methods to tokenize both queries and documents by processing the text into a list of tokens, each associated with its respective location (e.g., author, title, body, or query).\n",
                "\n",
                "The `preprocess()` method handles the following tasks:\n",
                "- **Text normalization**: Converts Unicode characters to ASCII.\n",
                "- **Punctuation removal**: Removes all punctuation marks.\n",
                "- **Number separation**: Separates numbers with spaces.\n",
                "- **Stopword elimination**: Removes common words that don\u2019t carry significant meaning.\n",
                "- **Stemming**: Reduces words to their root form using NLTK's SnowballStemmer.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.default_tokenizers ===\n",
                "\n",
                "import string\n",
                "import nltk\n",
                "import nltk.corpus\n",
                "import unidecode\n",
                "# from mir import DATA_DIR\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "# from mir.ir.token_ir import Token, TokenLocation\n",
                "\n",
                "\n",
                "class DefaultTokenizer(Tokenizer):\n",
                "    def __init__(self):\n",
                "        download_dir = f\"{DATA_DIR}/nltk_data\"\n",
                "        nltk.download(\"stopwords\", quiet=True, download_dir=download_dir,)\n",
                "        stopwords_from_path = nltk.data.find(\"corpora/stopwords/english\", [download_dir])\n",
                "        with open(stopwords_from_path) as f:\n",
                "            self.stopwords = frozenset(f.read().splitlines())\n",
                "        \n",
                "        self.stemmer = nltk.SnowballStemmer(\"english\")\n",
                "        self.remove_punctuation = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
                "        self.separate_numbers = str.maketrans({key: f\" {key} \" for key in string.digits})\n",
                "    \n",
                "    def preprocess(self, text: str):\n",
                "        # normalize unicode\n",
                "        text = unidecode.unidecode(text, errors=\"replace\", replace_str=\" \")\n",
                "        # replace punctuation with space\n",
                "        text = text.translate(self.remove_punctuation).lower()\n",
                "        # separate numbers with a space\n",
                "        text = text.translate(self.separate_numbers)\n",
                "        # split text into words\n",
                "        words = text.split()\n",
                "        # remove stopwords\n",
                "        words = [word for word in words if word not in self.stopwords]\n",
                "        # stem words\n",
                "        words: list[str] = [self.stemmer.stem(word) for word in words]\n",
                "        return words\n",
                "\n",
                "\n",
                "    def tokenize_query(self, query: str) -> list[Token]:\n",
                "        query_list = self.preprocess(query)\n",
                "        token_list = [Token(word, TokenLocation.QUERY) for word in query_list]\n",
                "        \n",
                "        return token_list\n",
                "\n",
                "    def tokenize_document(self, doc: DocumentContents) -> list[Token]:\n",
                "        author_list = self.preprocess(doc.author)\n",
                "        title_list = self.preprocess(doc.title)\n",
                "        body_list = self.preprocess(doc.body)\n",
                "        \n",
                "        token_list = \\\n",
                "            [Token(aword, TokenLocation.AUTHOR) for aword in author_list] + \\\n",
                "            [Token(tword, TokenLocation.TITLE) for tword in title_list] + \\\n",
                "            [Token(bword, TokenLocation.BODY) for bword in body_list]\n",
                "        \n",
                "        return token_list\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## MSMarco Dataset\n",
                "\n",
                "For this project, we chose the MSMarco dataset due to its large volume of data, which is ideal for fine-tuning a learning-to-rank model. \n",
                "\n",
                "Additionally, the dataset already includes predefined splits for training, validation, and testing, making it particularly well-suited for our needs.\n",
                "\n",
                "The following class automates data loading, preparing it for use with PyTorch in a learning-to-rank model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.neural_relevance.dataset ===\n",
                "\n",
                "from typing import Literal\n",
                "import torch\n",
                "from torch import nn\n",
                "# import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# from mir import DATA_DIR\n",
                "\n",
                "class MSMarcoDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, collection_path: str, queries_path: str, qrels_path: str):\n",
                "        self.collection = self.load_collection(collection_path)\n",
                "        self.queries = self.load_queries(queries_path)\n",
                "        self.qrels = self.load_qrels(qrels_path)\n",
                "\n",
                "    @staticmethod\n",
                "    def load(mode: Literal[\"train\", \"valid\", \"test\"]):\n",
                "        collection_path = f\"{DATA_DIR}/msmarco/collection.tsv\"\n",
                "        match mode:\n",
                "            case \"train\":\n",
                "                queries_path = f\"{DATA_DIR}/msmarco/queries.train.tsv\"\n",
                "                qrels_path = f\"{DATA_DIR}/msmarco/qrels.train.tsv\"\n",
                "            case \"valid\":\n",
                "                queries_path = f\"{DATA_DIR}/msmarco/msmarco-test2019-queries.tsv\"\n",
                "                qrels_path = f\"{DATA_DIR}/msmarco/2019qrels-pass.txt\"\n",
                "            case \"test\":\n",
                "                raise NotImplementedError(f\"Mode {mode} not implemented.\")\n",
                "            case _:\n",
                "                raise ValueError(f\"Invalid mode {mode}.\")\n",
                "        return MSMarcoDataset(collection_path, queries_path, qrels_path)\n",
                "\n",
                "    def load_collection(self, collection_path: str):\n",
                "        collection = pd.read_csv(collection_path, sep='\\t', header=None, names=['docid', 'text'], index_col='docid')\n",
                "        return collection\n",
                "    \n",
                "    def load_queries(self, queries_path: str):\n",
                "        queries = pd.read_csv(queries_path, sep='\\t', header=None, names=['qid', 'text'], index_col='qid')\n",
                "        return queries\n",
                "    \n",
                "    def load_qrels(self, qrels_path: str):\n",
                "        sep = ' ' if qrels_path.endswith(\".txt\") else '\\t'\n",
                "        qrels = pd.read_csv(qrels_path, sep=sep, header=None, names=['qid', 'Q0', 'docid', 'relevance'])\n",
                "        return qrels\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.qrels)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        qid = self.qrels.iloc[idx]['qid']\n",
                "        docid = self.qrels.iloc[idx]['docid']\n",
                "        relevance = self.qrels.iloc[idx]['relevance']\n",
                "        query = self.queries.loc[qid]['text']\n",
                "        doc = self.collection.loc[docid]['text']\n",
                "        return query, doc, relevance\n",
                "    \n",
                "    @staticmethod\n",
                "    def collate_fn(batch):\n",
                "        queries, docs, relevances = zip(*batch)\n",
                "        return queries, docs, torch.tensor(relevances, dtype=torch.float32)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## BERT Learning-to-Rank\n",
                "\n",
                "The `NeuralRelevance` class uses a pre-trained BERT model for learning-to-rank tasks, where the BERT model's weights are frozen, and only a \"similarity head\" is fine-tuned. \n",
                "\n",
                "The model is trained with a binary cross-entropy loss function, which measures the discrepancy between predicted and actual relevance scores for queries and documents.\n",
                "\n",
                "The input is preprocessed and encoded as follows:\n",
                "`[CLS] Query [SEP] Document [SEP]`\n",
                "\n",
                "Key features:\n",
                "- Training is performed on the MSMarco dataset with early stopping to prevent overfitting.\n",
                "- Model saving, loading, and the ability to download pre-trained weights from a URL are included.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.neural_relevance.model ===\n",
                "\n",
                "# import os\n",
                "import requests\n",
                "# import torch\n",
                "from torch import nn\n",
                "from tqdm.auto import tqdm\n",
                "import transformers\n",
                "\n",
                "# from mir import DATA_DIR\n",
                "# from mir.neural_relevance.dataset import MSMarcoDataset\n",
                "\n",
                "class NeuralRelevance(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.device = torch.device(\n",
                "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        \n",
                "        model_name = \"bert-large-uncased\"\n",
                "        self.tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
                "        self.model = transformers.BertModel.from_pretrained(model_name).to(self.device)\n",
                "        for param in self.model.parameters():\n",
                "            param.requires_grad = False\n",
                "        \n",
                "        bert_embedding_size = self.model.config.hidden_size\n",
                "        self.similairty_head = nn.Sequential(\n",
                "            nn.Linear(bert_embedding_size, 1, device=self.device),\n",
                "            nn.Sigmoid()\n",
                "        ).to(self.device)\n",
                "        \n",
                "\n",
                "    def forward(self, x: dict) -> torch.Tensor:\n",
                "        x = self.model(**x).last_hidden_state\n",
                "        features = x[:, 0, :]\n",
                "        x = self.similairty_head(features)\n",
                "        return x.squeeze()\n",
                "\n",
                "    def preprocess(self, queries: list[str], documents: list[str]) -> dict:\n",
                "        tokens = self.tokenizer(queries, documents, return_tensors=\"pt\", padding=True).to(self.device)\n",
                "        return tokens\n",
                "\n",
                "    def forward_queries_and_documents(self, queries: list[str], documents: list[str]) -> torch.Tensor:\n",
                "        x = self.preprocess(queries, documents)\n",
                "        return self.forward(x)\n",
                "\n",
                "    def loss(\n",
                "        self,\n",
                "        similarity: torch.Tensor,\n",
                "        relevance: torch.Tensor,\n",
                "    ):\n",
                "        ce_similarity_loss = torch.nn.functional.binary_cross_entropy(similarity, relevance / 5)\n",
                "        mse_similarity_loss = torch.nn.functional.mse_loss(similarity, relevance / 5)\n",
                "        return ce_similarity_loss, mse_similarity_loss, ce_similarity_loss\n",
                "\n",
                "    def fit(self, train: MSMarcoDataset, valid: MSMarcoDataset, epochs: int = 100):\n",
                "        bs = 16\n",
                "        train_loader = torch.utils.data.DataLoader(\n",
                "            train,\n",
                "            batch_size=bs,\n",
                "            collate_fn=MSMarcoDataset.collate_fn,\n",
                "            sampler=torch.utils.data.RandomSampler(\n",
                "                train, replacement=True, num_samples=bs * 100)\n",
                "        )\n",
                "        valid_loader = torch.utils.data.DataLoader(\n",
                "            valid,\n",
                "            batch_size=bs,\n",
                "            collate_fn=MSMarcoDataset.collate_fn,\n",
                "            sampler=torch.utils.data.RandomSampler(\n",
                "                valid, replacement=True, num_samples=bs * 50)\n",
                "        )\n",
                "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1)\n",
                "        best_loss = float(\"inf\")\n",
                "        best_model = None\n",
                "        patience = 3\n",
                "        threshold = 0.001\n",
                "        epochs_without_improvement = 0\n",
                "\n",
                "        history = {\n",
                "            \"train_ce\": [], \"valid_ce\": [],\n",
                "            \"train_mse\": [], \"valid_mse\": [],\n",
                "        }\n",
                "\n",
                "        for epoch in range(epochs):\n",
                "            self.train()\n",
                "            avg_ce = 0\n",
                "            avg_mse = 0\n",
                "            batches = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} (train)\", total=len(train_loader))\n",
                "            for i, (queries, docs, relevances) in enumerate(batches):\n",
                "                relevances = relevances.to(self.device)\n",
                "                optimizer.zero_grad()\n",
                "                similarity = self.forward_queries_and_documents(queries, docs)\n",
                "                ce, mse, loss = self.loss(similarity, relevances)\n",
                "                avg_ce += ce.item()\n",
                "                avg_mse += mse.item()\n",
                "                batches.set_postfix(\n",
                "                    ce=avg_ce / (i + 1), \n",
                "                    mse=avg_mse / (i + 1))\n",
                "                loss.backward()\n",
                "                torch.nn.utils.clip_grad_norm_(self.parameters(), 1)\n",
                "                optimizer.step()\n",
                "            avg_ce /= (i + 1)\n",
                "            avg_mse /= (i + 1)\n",
                "            history[\"train_ce\"].append(avg_ce)\n",
                "            history[\"train_mse\"].append(avg_mse)\n",
                "            self.eval()\n",
                "            with torch.no_grad():\n",
                "                avg_ce = 0\n",
                "                avg_mse = 0\n",
                "                avg_loss = 0\n",
                "                batches = tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{epochs} (valid)\", total=len(valid_loader))\n",
                "                for i, (queries, docs, relevances) in enumerate(batches):\n",
                "                    relevances = relevances.to(self.device)\n",
                "                    similarity = self.forward_queries_and_documents(queries, docs)\n",
                "                    ce, mse, loss = self.loss(similarity, relevances)\n",
                "                    avg_ce += ce.item()\n",
                "                    avg_mse += mse.item()\n",
                "                    avg_loss += loss.item()\n",
                "                    batches.set_postfix(\n",
                "                        ce=avg_ce / (i + 1),\n",
                "                        mse=avg_mse / (i + 1))\n",
                "                avg_ce /= (i + 1)\n",
                "                avg_mse /= (i + 1)\n",
                "                history[\"valid_ce\"].append(avg_ce)\n",
                "                history[\"valid_mse\"].append(avg_mse)\n",
                "                if avg_loss < best_loss - threshold:\n",
                "                    best_loss = avg_loss\n",
                "                    best_model = self.state_dict()\n",
                "                    epochs_without_improvement = 0\n",
                "                else:\n",
                "                    epochs_without_improvement += 1\n",
                "                    if epochs_without_improvement >= patience:\n",
                "                        break\n",
                "        self.load_state_dict(best_model)\n",
                "        return history\n",
                "    \n",
                "    def save(self, path: str):\n",
                "        torch.save(self.state_dict(), path)\n",
                "    \n",
                "    @staticmethod\n",
                "    def load(path: str):\n",
                "        model = NeuralRelevance()\n",
                "        model.load_state_dict(torch.load(path, map_location=model.device, weights_only=True))\n",
                "        return model\n",
                "    \n",
                "    @staticmethod\n",
                "    def from_pretrained():\n",
                "        if not os.path.exists(f\"{DATA_DIR}/neural_relevance.pt\"):\n",
                "            url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/neural_relevance.pt\"\n",
                "            weights_request = requests.get(url)\n",
                "            weights_request.raise_for_status()\n",
                "            with tqdm(total=int(weights_request.headers[\"Content-Length\"]), unit=\"B\", unit_scale=True, desc=\"Downloading weights\") as pbar:\n",
                "                with open(f\"{DATA_DIR}/neural_relevance.pt\", \"wb\") as f:\n",
                "                    for chunk in weights_request.iter_content(chunk_size=1024):\n",
                "                        f.write(chunk)\n",
                "                        pbar.update(len(chunk))\n",
                "        model = NeuralRelevance.load(f\"{DATA_DIR}/neural_relevance.pt\")\n",
                "        return model\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Neural Scoring Function\n",
                "\n",
                "The `NeuralScoringFunction` class utilizes the pre-trained `NeuralRelevance` model to compute relevance scores between queries and documents. \n",
                "\n",
                "It provides two methods: the `__call__()` method calculates the score for a single document-query pair, while the `batched_call()` method processes multiple documents against a single query in batch mode. \n",
                "\n",
                "The model is used in evaluation mode, with gradients disabled to enhance performance. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.neural_scoring_function ===\n",
                "\n",
                "import numpy as np\n",
                "# import torch\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# from mir import DATA_DIR\n",
                "# from mir.neural_relevance.model import NeuralRelevance\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.scoring_function import ScoringFunction\n",
                "# from mir.ir.term import Term\n",
                "# from mir.neural_relevance.dataset import MSMarcoDataset\n",
                "\n",
                "\n",
                "class NeuralScoringFunction(ScoringFunction):\n",
                "    def __init__(self):\n",
                "        # Load the model\n",
                "        self.model = NeuralRelevance.from_pretrained()\n",
                "        self.model.eval()\n",
                "\n",
                "    def __call__(self, document: DocumentInfo, postings: list[Posting], query: list[Term], *, document_content: str, query_content: str, **kwargs) -> float:\n",
                "        if len(document_content) == 0 or len(query_content) == 0:\n",
                "            return 0.0\n",
                "        with torch.no_grad():\n",
                "            score = self.model.forward_queries_and_documents([query_content], [document_content])\n",
                "        return score.item()\n",
                "    \n",
                "    def batched_call(self, document_contents: list[str], query_contents: str) -> list[float]:\n",
                "        scores = []\n",
                "        with torch.no_grad():\n",
                "            scores = self.model.forward_queries_and_documents([query_contents]*len(document_contents), document_contents)\n",
                "        return scores.tolist()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## SQLite Index \n",
                "\n",
                "The `SqliteIndex` class is an implementation of an inverted index based on SQLite, designed to manage the indexing of documents in a search system. The index stores various information in its database, including postings, terms, documents, and document metadata.\n",
                "\n",
                "SQLite was chosen for its efficient disk management, enabling asynchronous writes, multi-threading, and effective caching, which allows handling large amounts of data efficiently.\n",
                "\n",
                "The `global_info` table is continuously updated during the indexing process. This information is used to compute the average length of various fields, which plays a crucial role in optimizing search efficiency.\n",
                "\n",
                "The class includes methods to retrieve this global data, efficiently caching it to enhance performance during searches.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.impls.sqlite_index ===\n",
                "\n",
                "from collections.abc import Generator\n",
                "# import os\n",
                "import sqlite3\n",
                "import sys\n",
                "from typing import Any, Optional\n",
                "\n",
                "import psutil\n",
                "# from mir.ir.document_info import DocumentInfo\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.impls.default_tokenizers import DefaultTokenizer\n",
                "# from mir.ir.index import Index\n",
                "# from mir.ir.posting import Posting\n",
                "# from mir.ir.term import Term\n",
                "# from mir.ir.token_ir import TokenLocation\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "\n",
                "\n",
                "class SqliteIndex(Index):\n",
                "    def __init__(self, path: Optional[str] = None):\n",
                "        super().__init__()\n",
                "\n",
                "        self.connection = sqlite3.connect(\n",
                "            path if path is not None else \":memory:\", \n",
                "            check_same_thread=False, \n",
                "            cached_statements=1024,)\n",
                "        \n",
                "        assert sys.version_info.major == 3, \"Python 2 is not supported\"\n",
                "        assert sys.version_info.minor >= 10, \"Python <3.10 is not supported\"\n",
                "        \n",
                "        legacy_mode = sys.version_info.minor == 10\n",
                "        if legacy_mode:\n",
                "            self.isolation_level = None\n",
                "        else:\n",
                "            self.connection.autocommit = True\n",
                "        \n",
                "        self.connection.execute(\"pragma synchronous = off\")\n",
                "        self.connection.execute(f\"pragma threads = {os.cpu_count()}\")\n",
                "        self.connection.execute(\"pragma journal_mode = WAL\")\n",
                "        cache_memory = psutil.virtual_memory().total // 1024 // 2\n",
                "        self.connection.execute(f\"pragma cache_size = {-cache_memory}\")\n",
                "        self.connection.execute(f\"pragma mmap_size = {1024*1024*1024 * 16}\")\n",
                "        self.connection.execute(\"pragma temp_store = memory\")\n",
                "\n",
                "        if legacy_mode:\n",
                "            self.connection.isolation_level = \"DEFERRED\"\n",
                "        else:\n",
                "            self.connection.autocommit = False\n",
                "\n",
                "\n",
                "        self.connection.execute(\n",
                "            \"create table if not exists postings \"\n",
                "            \"(term_id integer references terms(term_id) not null, \"\n",
                "            \"doc_id integer references document_info(doc_id) not null, \"\n",
                "            \"occurrences_author integer not null, \"\n",
                "            \"occurrences_title integer not null, \"\n",
                "            \"occurrences_body integer not null, \"\n",
                "            \"primary key (term_id, doc_id))\")\n",
                "        self.connection.execute(\n",
                "            \"create table if not exists document_info \"\n",
                "            \"(doc_id integer not null primary key autoincrement, \"\n",
                "            \"author_len integer not null, \"\n",
                "            \"title_len integer not null, \"\n",
                "            \"body_len integer not null)\")\n",
                "        self.connection.execute(\n",
                "            \"create table if not exists document_contents \"\n",
                "            \"(doc_id integer not null primary key references document_info(doc_id), \"\n",
                "            \"author text, \"\n",
                "            \"title text, \"\n",
                "            \"body text)\")\n",
                "        self.connection.execute(\n",
                "            \"create table if not exists terms \"\n",
                "            \"(term_id integer not null primary key autoincrement, \"\n",
                "            \"term text unique not null, \"\n",
                "            \"document_frequency integer not null)\")\n",
                "        \n",
                "        self.connection.execute(\"create table if not exists global_info (key text not null primary key, value integer)\")\n",
                "        # add global info default values if not present\n",
                "        self.connection.execute(\"insert or ignore into global_info values ('total_author_len', 0)\")\n",
                "        self.connection.execute(\"insert or ignore into global_info values ('total_title_len', 0)\")\n",
                "        self.connection.execute(\"insert or ignore into global_info values ('total_body_len', 0)\")\n",
                "        self.connection.execute(\"insert or ignore into global_info values ('num_docs', 0)\")\n",
                "\n",
                "        self.connection.execute(\"pragma optimize\")\n",
                "\n",
                "        self.connection.commit()\n",
                "        self.global_info_dirty = True \n",
                "        self.cached_global_info = None\n",
                "    \n",
                "    def get_postings(self, term_id: int) -> Generator[Posting, None, None]:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\n",
                "            \"select doc_id, occurrences_author, occurrences_title, occurrences_body from postings where term_id = ? \"\n",
                "            \"order by doc_id\", (term_id,))\n",
                "        def row_factory(_cursor, row):\n",
                "            return Posting(row[0], term_id, {\"author\": row[1], \"title\": row[2], \"body\": row[3]})\n",
                "        cursor.row_factory = row_factory\n",
                "        yield from cursor\n",
                "\n",
                "    def get_document_info(self, doc_id: int) -> DocumentInfo:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select author_len, title_len, body_len from document_info where doc_id = ?\", (doc_id,))\n",
                "        author_len, title_len, body_len = cursor.fetchone()\n",
                "        return DocumentInfo(doc_id, [author_len, title_len, body_len])\n",
                "    \n",
                "    def get_document_contents(self, doc_id: int) -> DocumentContents:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select author, title, body from document_contents where doc_id = ?\", (doc_id,))\n",
                "        author, title, body = cursor.fetchone()\n",
                "        return DocumentContents(author, title, body)\n",
                "\n",
                "    def get_term(self, term_id: int) -> Term:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select term, document_frequency from terms where term_id = ?\", (term_id,))\n",
                "        term, document_frequency = cursor.fetchone()\n",
                "        return Term(term, term_id, document_frequency=document_frequency)\n",
                "\n",
                "    def get_term_id(self, term: str) -> Optional[int]:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select term_id from terms where term = ?\", (term,))\n",
                "        result = cursor.fetchone()\n",
                "        return result[0] if result is not None else None\n",
                "    \n",
                "    def get_global_info(self) -> dict[str, Any]:\n",
                "        if self.global_info_dirty:\n",
                "            cursor = self.connection.cursor()\n",
                "            cursor.execute(\"select key, value from global_info\")\n",
                "            global_info = cursor.fetchall()\n",
                "            global_info = {key: value for key, value in global_info}\n",
                "            self.cached_global_info = {\n",
                "                \"avg_field_lengths\": {\n",
                "                    \"author\": global_info[\"total_author_len\"] / global_info[\"num_docs\"],\n",
                "                    \"title\": global_info[\"total_title_len\"] / global_info[\"num_docs\"],\n",
                "                    \"body\": global_info[\"total_body_len\"] / global_info[\"num_docs\"]\n",
                "                },\n",
                "                \"num_docs\": global_info[\"num_docs\"]\n",
                "            }\n",
                "            self.global_info_dirty = False\n",
                "        return self.cached_global_info\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select value from global_info where key = 'num_docs'\")\n",
                "        return cursor.fetchone()[0]\n",
                "\n",
                "    def _increment_field_lengths(self, author_len: int, title_len: int, body_len: int) -> None:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"update global_info set value = value + ? where key = 'total_author_len'\", (author_len,))\n",
                "        cursor.execute(\"update global_info set value = value + ? where key = 'total_title_len'\", (title_len,))\n",
                "        cursor.execute(\"update global_info set value = value + ? where key = 'total_body_len'\", (body_len,))\n",
                "\n",
                "    def _create_or_get_term_id(self, term: str) -> int:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"insert or ignore into terms(term, document_frequency) values (?, 0)\", (term,))\n",
                "        cursor.execute(\"select term_id from terms where term = ?\", (term,))\n",
                "        return cursor.fetchone()[0]\n",
                "\n",
                "    def _new_document(self, doc: DocumentContents, author_len: int, title_len: int, body_len: int) -> int:\n",
                "        cursor = self.connection.cursor()\n",
                "        if doc.__dict__.get(\"doc_id\") is not None:\n",
                "            cursor.execute(\"insert into document_info(doc_id, author_len, title_len, body_len) values (?, ?, ?, ?)\", (doc.doc_id, author_len, title_len, body_len))\n",
                "        else:\n",
                "            cursor.execute(\"insert into document_info(author_len, title_len, body_len) values (?, ?, ?)\", (author_len, title_len, body_len))\n",
                "        doc_id = cursor.lastrowid\n",
                "        cursor.execute(\"insert into document_contents(doc_id, author, title, body) values (?, ?, ?, ?)\", (doc_id, doc.author, doc.title, doc.body))\n",
                "        cursor.execute(\"update global_info set value = value + 1 where key = 'num_docs'\")\n",
                "        return doc_id\n",
                "\n",
                "    def _update_postings(self, term_id: int, doc_id: int, location: TokenLocation) -> None:\n",
                "        increments = {\n",
                "            \"author\": 1 if location == TokenLocation.AUTHOR else 0, \n",
                "            \"title\": 1 if location == TokenLocation.TITLE else 0, \n",
                "            \"body\": 1 if location == TokenLocation.BODY else 0\n",
                "        }\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select occurrences_author, occurrences_title, occurrences_body from postings where term_id = ? and doc_id = ?\", (term_id, doc_id))\n",
                "        result = cursor.fetchone()\n",
                "        if result is None:\n",
                "            cursor.execute(\n",
                "                \"insert into postings(term_id, doc_id, occurrences_author, occurrences_title, occurrences_body) \"\n",
                "                \"values (?, ?, ?, ?, ?)\", (term_id, doc_id, increments[\"author\"], increments[\"title\"], increments[\"body\"]))\n",
                "        else:\n",
                "            cursor.execute(\n",
                "                \"update postings set occurrences_author = occurrences_author + ?, occurrences_title = occurrences_title + ?, \"\n",
                "                \"occurrences_body = occurrences_body + ? where term_id = ? and doc_id = ?\",\n",
                "                (increments[\"author\"], increments[\"title\"], increments[\"body\"], term_id, doc_id))\n",
                "\n",
                "    def _contains_document(self, doc_id: int) -> bool:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"select count(*) from document_info where doc_id = ?\", (doc_id,))\n",
                "        ret = cursor.fetchone()[0]\n",
                "        return ret > 0\n",
                "\n",
                "    def _increment_document_frequency(self, term_id: int) -> None:\n",
                "        cursor = self.connection.cursor()\n",
                "        cursor.execute(\"update terms set document_frequency = document_frequency + 1 where term_id = ?\", (term_id,))\n",
                "\n",
                "    def index_document(self, doc: DocumentContents, tokenizer: Tokenizer) -> None:\n",
                "\n",
                "        if doc.__dict__.get(\"doc_id\") is not None:\n",
                "            if self._contains_document(doc.doc_id):\n",
                "                return\n",
                "        self.global_info_dirty = True\n",
                "\n",
                "        terms = tokenizer.tokenize_document(doc)\n",
                "        author_length = sum(1 for term in terms if term.location == TokenLocation.AUTHOR)\n",
                "        title_length = sum(1 for term in terms if term.location == TokenLocation.TITLE)\n",
                "        body_length = sum(1 for term in terms if term.location == TokenLocation.BODY)\n",
                "        \n",
                "        self._increment_field_lengths(author_length, title_length, body_length)\n",
                "\n",
                "        encountered_terms = set()\n",
                "        term_ids_and_locations = []\n",
                "        for term in terms:\n",
                "            term_id = self._create_or_get_term_id(term.text)\n",
                "            if term_id not in encountered_terms:\n",
                "                self._increment_document_frequency(term_id)\n",
                "            encountered_terms.add(term_id)\n",
                "            term_ids_and_locations.append((term_id, term.location))\n",
                "        doc_id = self._new_document(doc, author_length, title_length, body_length)\n",
                "        for term_id, location in term_ids_and_locations:\n",
                "            self._update_postings(term_id, doc_id, location)\n",
                "        self.connection.commit()\n",
                "\n",
                "    def bulk_index_documents(self, docs, tokenizer, verbose = False):\n",
                "        super().bulk_index_documents(docs, tokenizer, verbose)\n",
                "        self.connection.execute(\"pragma optimize\")\n",
                "        self.connection.commit()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Priority Queue\n",
                "\n",
                "The `PriorityQueue` class implements a priority queue using a heap (via Python's heapq module).\n",
                "\n",
                "The `PriorityQueue` is used to manage and sort documents based on their scores during a search. It ensures efficient ordering of results.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.priority_queue ===\n",
                "\n",
                "import heapq\n",
                "from typing import Iterable, Optional, Sized\n",
                "\n",
                "class PriorityQueue(Iterable[tuple[float, int]], Sized):\n",
                "    def __init__(self, max_size: int):\n",
                "        \"\"\"\n",
                "        Create a priority queue with a maximum size.\n",
                "        \"\"\"\n",
                "        self.heap = []\n",
                "        self.finalised = False\n",
                "        self.max_size = max_size\n",
                "    \n",
                "    def push(self, doc_id: int, score: float) -> Optional[int]:\n",
                "        \"\"\"\n",
                "        Add an item with a given score to the priority queue\n",
                "\n",
                "        Returns the doc_id of the item that was popped, if any. If the new item was not added returns its doc_id.\n",
                "        \"\"\"\n",
                "        if len(self) == self.max_size:\n",
                "            if score > self.heap[0][0]:\n",
                "                popped = heapq.heappushpop(self.heap, (score, doc_id))\n",
                "                return popped[1]\n",
                "            else:\n",
                "                return doc_id\n",
                "        else:\n",
                "            heapq.heappush(self.heap, (score, doc_id))\n",
                "            return None\n",
                "    \n",
                "    def finalise(self):\n",
                "        \"\"\"\n",
                "        Call this after all items have been pushed to the priority queue.\n",
                "        \"\"\"\n",
                "        self.heap.sort(reverse=True)\n",
                "        self.finalised = True\n",
                "    \n",
                "    def __iter__(self) -> Iterable[tuple[float, int]]:\n",
                "        \"\"\"\n",
                "        Iterate over the items in the priority queue.\n",
                "        \"\"\"\n",
                "        if not self.finalised:\n",
                "            raise ValueError(\"Priority queue must be finalised before iterating\")\n",
                "        return iter(self.heap)\n",
                "    \n",
                "    def __len__(self) -> int:\n",
                "        \"\"\"\n",
                "        Get the number of items in the priority queue.\n",
                "        \"\"\"\n",
                "        return len(self.heap)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## IR System\n",
                "\n",
                "The `IR` class serves as the backbone of the project, integrating all necessary components to create a fully functional instance. It orchestrates indexing and search operations by leveraging these components.\n",
                "\n",
                "### Initialization\n",
                "During the initialization of the IR System, a **scoring function pipeline** can be provided. This pipeline consists of a list of tuples in the format:  \n",
                "\n",
                "**`(number of documents, scoring function)`**\n",
                "\n",
                "- The **first element** specifies the total number of final results to be returned.  \n",
                "- The **subsequent elements** define how many documents should be re-ranked by each scoring function in the pipeline.  \n",
                "\n",
                "This flexible design enables efficient ranking and re-ranking workflows tailored to various information retrieval tasks.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.ir.ir ===\n",
                "\n",
                "from collections.abc import Generator\n",
                "# import string\n",
                "import time\n",
                "from typing import Optional\n",
                "\n",
                "# import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "from more_itertools import peekable\n",
                "\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.ir.impls.default_index import DefaultIndex\n",
                "# from mir.ir.impls.count_scoring_function import CountScoringFunction\n",
                "# from mir.ir.impls.default_tokenizers import DefaultTokenizer\n",
                "# from mir.ir.index import Index\n",
                "# from mir.ir.priority_queue import PriorityQueue\n",
                "# from mir.ir.scoring_function import ScoringFunction\n",
                "# from mir.ir.tokenizer import Tokenizer\n",
                "# from mir.utils.sized_generator import SizedGenerator\n",
                "\n",
                "class Ir:\n",
                "    def __init__(self, index: Optional[Index] = None, tokenizer: Optional[Tokenizer] = None, scoring_functions: Optional[list[tuple[int, ScoringFunction]]] = None):\n",
                "        \"\"\"\n",
                "        Create an IR system.\n",
                "\n",
                "        # Parameters\n",
                "        - index (Index): The index to use. If None, a DefaultIndex is used.\n",
                "        - tokenizer (Tokenizer): The tokenizer to use. If None, a DefaultTokenizer is used.\n",
                "        - scoring_functions (Optional[list[tuple[int, ScoringFunction]]]): A list of scoring functions to use, with their respective top_k results to keep.\n",
                "        If None CountScoringFunction is used.\n",
                "        \"\"\"\n",
                "        self.index: Index = index if index is not None else DefaultIndex()\n",
                "        self.tokenizer: Tokenizer = tokenizer if tokenizer is not None else DefaultTokenizer()\n",
                "        self.scoring_functions: list[tuple[int, ScoringFunction]] = scoring_functions if scoring_functions is not None else [\n",
                "            (1000, CountScoringFunction())\n",
                "        ]\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        \"\"\"\n",
                "        Get the number of documents in the index.\n",
                "        \"\"\"\n",
                "        return len(self.index)\n",
                "\n",
                "    def index_document(self, doc: DocumentContents) -> None:\n",
                "        \"\"\"\n",
                "        Index a document.\n",
                "\n",
                "        # Parameters\n",
                "        - doc (DocumentContents): The document to index.\n",
                "        \"\"\"\n",
                "        self.index.index_document(doc, self.tokenizer)\n",
                "\n",
                "    def bulk_index_documents(self, docs: SizedGenerator[DocumentContents, None, None], verbose: bool = False) -> None:\n",
                "        \"\"\"\n",
                "        Bulk index documents.\n",
                "\n",
                "        # Parameters\n",
                "        - docs (SizedGenerator[DocumentContents, None, None]): A generator of documents to index.\n",
                "        - verbose (bool): Whether to show a progress bar.\n",
                "        \"\"\"\n",
                "        self.index.bulk_index_documents(docs, self.tokenizer, verbose)\n",
                "\n",
                "    def search(self, query: str) -> Generator[DocumentContents, None, None]:\n",
                "        \"\"\"\n",
                "        Search for documents based on a query.\n",
                "        Uses document-at-a-time scoring.\n",
                "\n",
                "        # Parameters\n",
                "        - query (str): The query to search for.\n",
                "        - scoring_functions (list[ScoringFunction]): A list of scoring functions to use.\n",
                "\n",
                "        # Yields\n",
                "        - DocumentContents: A document that matches the query. In decreasing order of score.\n",
                "        It also has a score attribute with the score of the document.\n",
                "        \"\"\"\n",
                "\n",
                "        assert len(self.scoring_functions) > 0, \"At least one scoring function must be provided\"\n",
                "\n",
                "        ks, scoring_functions = zip(*self.scoring_functions)\n",
                "        scoring_functions: list[ScoringFunction] = list(scoring_functions)\n",
                "        ks: list[int] = list(ks)[::-1]\n",
                "        \n",
                "        terms = self.tokenizer.tokenize_query(query)\n",
                "        term_ids = [term_id for term in terms if (\n",
                "            term_id := self.index.get_term_id(term.text)) is not None]\n",
                "        terms = [self.index.get_term(term_id) for term_id in term_ids]\n",
                "        posting_generators = [\n",
                "            peekable(self.index.get_postings(term_id)) for term_id in term_ids]\n",
                "\n",
                "        priority_queue = PriorityQueue(ks[-1])\n",
                "        first_scoring_function = scoring_functions[0]\n",
                "        postings_cache = {}\n",
                "\n",
                "        while True:\n",
                "            # find the lowest doc_id among all the posting lists\n",
                "            # doing this avoids having to iterate over all the doc_ids\n",
                "            # we only take into account the doc_ids that are present in the posting lists\n",
                "            lowest_doc_id = None\n",
                "            empty_posting_lists = []\n",
                "            for i, posting in enumerate(posting_generators):\n",
                "                try:\n",
                "                    doc_id = posting.peek().doc_id\n",
                "                    if lowest_doc_id is None or doc_id < lowest_doc_id:\n",
                "                        lowest_doc_id = doc_id\n",
                "                except StopIteration:\n",
                "                    empty_posting_lists.append(i)\n",
                "            # all the posting lists are empty\n",
                "            if lowest_doc_id is None:\n",
                "                break\n",
                "\n",
                "            # remove the empty posting lists\n",
                "            for i in reversed(empty_posting_lists):\n",
                "                posting_generators.pop(i)\n",
                "                term_ids.pop(i)\n",
                "\n",
                "            postings = []\n",
                "            # get all the postings with the current doc_id, and advance their iterators\n",
                "            for i, posting in enumerate(posting_generators):\n",
                "                if posting.peek().doc_id == lowest_doc_id:\n",
                "                    next_posting = next(posting)\n",
                "                    postings.append(next_posting)\n",
                "            postings_cache[lowest_doc_id] = postings\n",
                "            # now that we have all the info about the current document, we can score it\n",
                "            global_info = self.index.get_global_info()\n",
                "            document_info = self.index.get_document_info(lowest_doc_id)\n",
                "            score = first_scoring_function(document_info, postings, terms, **global_info)\n",
                "            # we add the score and doc_id to the priority queue\n",
                "            popped_doc_id = priority_queue.push(lowest_doc_id, score)\n",
                "            # if the priority queue is full, we remove the lowest score\n",
                "            if popped_doc_id is not None:\n",
                "                del postings_cache[popped_doc_id]\n",
                "        \n",
                "        priority_queue.finalise()\n",
                "\n",
                "        for scoring_function in scoring_functions[1:]:\n",
                "            ks.pop()\n",
                "            resorted_documents = []\n",
                "            if scoring_function.batched_call is not None:\n",
                "                scores: list[float] = scoring_function.batched_call(\n",
                "                    [self.index.get_document_contents(doc_id).body for _, doc_id in priority_queue.heap[:ks[-1]]],\n",
                "                    query\n",
                "                )\n",
                "                for i, (score, doc_id) in enumerate(priority_queue.heap[:ks[-1]]):\n",
                "                    new_score = scores[i]\n",
                "                    resorted_documents.append((new_score + score, doc_id))\n",
                "            else:\n",
                "                for score, doc_id in priority_queue.heap[:ks[-1]]:\n",
                "                    postings = postings_cache[doc_id]\n",
                "                    global_info = self.index.get_global_info()\n",
                "                    global_info[\"document_content\"] = self.index.get_document_contents(doc_id).body\n",
                "                    global_info[\"query_content\"] = query\n",
                "                    new_score = scoring_function(self.index.get_document_info(doc_id), postings, terms, **global_info)\n",
                "                    # we add the old score to maintain monotonicity\n",
                "                    resorted_documents.append((new_score + score, doc_id))\n",
                "            \n",
                "            resorted_documents.sort(key=lambda x: x[0], reverse=True)\n",
                "            priority_queue.heap = resorted_documents + priority_queue.heap[ks[-1]:]\n",
                "\n",
                "        for score, doc_id in priority_queue:\n",
                "            ret = self.index.get_document_contents(doc_id)\n",
                "            ret.add_field(\"id\", doc_id)\n",
                "            ret.set_score(score)\n",
                "            yield ret\n",
                "\n",
                "    def get_run(self, queries: pd.DataFrame, verbose: bool = False, pyterrier_compatible: bool = False) -> pd.DataFrame:\n",
                "        \"\"\"\n",
                "        Generate a run file for the given queries in the form of a pandas DataFrame.\n",
                "        You can encode it to a file using a tab separator and the to_csv method.\n",
                "\n",
                "        # Parameters\n",
                "        - queries (pd.DataFrame): A DataFrame with the queries to run. \n",
                "        It must have the columns \"query_id\" and \"text\".\n",
                "        - verbose (bool): Whether to show a progress bar.\n",
                "\n",
                "        # Returns\n",
                "        - pd.DataFrame: The run file. It has the columns \n",
                "        \"query_id\", \"Q0\", \"document_no\", \"rank\", \"score\", \"run_id\".\n",
                "        If pyterrier_compatible is True, the columns are \"qid\", \"docid\", \"docno\", \"rank\", \"score\", \"query\".\n",
                "        \"\"\"\n",
                "        \n",
                "        run = []\n",
                "        for _, query_row in tqdm(queries.iterrows(), desc=\"Running queries\", disable=not verbose, total=len(queries)):\n",
                "            query_id = query_row[\"query_id\"]\n",
                "            query = query_row[\"text\"]\n",
                "            for rank, doc in enumerate(self.search(query), start=0):\n",
                "                if pyterrier_compatible:\n",
                "                    run.append(\n",
                "                        {\"qid\": query_id, \"docid\": doc.id, \"docno\": doc.id, \"rank\": rank, \"score\": doc.score, \"query\": query})\n",
                "                else:\n",
                "                    run.append(\n",
                "                        {\"query_id\": query_id, \"Q0\": \"Q0\", \"doc_id\": doc.id, \"rank\": rank, \"score\": doc.score, \"run_id\": self.__class__.__name__})\n",
                "\n",
                "        run = pd.DataFrame(run)\n",
                "        return run\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Dataset\n",
                "\n",
                "This code provides functions for downloading and processing the MS MARCO dataset:\n",
                "\n",
                "1. **`get_msmarco_dataset`**:\n",
                "   - Downloads various components of the MS MARCO dataset (such as the corpus, queries, and relevance judgments) to a specified directory.\n",
                "   - Handles downloading, file extraction (both `.tar.gz` and `.gz` formats), and ensures that files are only downloaded once.\n",
                "\n",
                "2. **`msmarco_dataset_to_contents`**:\n",
                "   - Converts the corpus from the MS MARCO dataset (stored in a pandas DataFrame) into a generator of `DocumentContents` objects, which includes the document text (`body`) and document ID (`doc_id`).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.utils.dataset ===\n",
                "\n",
                "from collections.abc import Generator\n",
                "import gzip\n",
                "import tarfile\n",
                "# import requests\n",
                "# from mir import DATA_DIR\n",
                "# import pandas as pd\n",
                "# import os\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# from mir.ir.document_contents import DocumentContents\n",
                "# from mir.utils.sized_generator import SizedGenerator\n",
                "\n",
                "def get_msmarco_dataset(verbose: bool = False):\n",
                "    \"\"\"\n",
                "    Downloads the MS MARCO dataset to the data directory.\n",
                "    \"\"\"\n",
                "    corpus = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz\"\n",
                "    queries = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\"\n",
                "    queries_valid = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2019-queries.tsv.gz\"\n",
                "    queries_test = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-test2020-queries.tsv.gz\"\n",
                "    qrels_train = \"https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.train.tsv\"\n",
                "    # trec link is usually down so I'm using my own link to the same files\n",
                "    # qrels_valid = \"https://trec.nist.gov/data/deep/2019qrels-pass.txt\"\n",
                "    # qrels_test = \"https://trec.nist.gov/data/deep/2020qrels-pass.txt\"\n",
                "    qrels_valid = \"https://huggingface.co/Etto48/MIRProject/resolve/main/2019qrels-pass.txt\"\n",
                "    qrels_test = \"https://huggingface.co/Etto48/MIRProject/resolve/main/2020qrels-pass.txt\"\n",
                "    \n",
                "    urls = [corpus, queries, queries_valid, queries_test, qrels_train, qrels_valid, qrels_test]\n",
                "    dataset_dir = f\"{DATA_DIR}/msmarco\"\n",
                "    os.makedirs(dataset_dir, exist_ok=True)\n",
                "    for url in urls:\n",
                "        file_name = url.split(\"/\")[-1]\n",
                "        path = f\"{dataset_dir}/{file_name}\"\n",
                "        if not os.path.exists(path):\n",
                "            response = requests.get(url, stream=True)\n",
                "            response.raise_for_status()\n",
                "            file_size = int(response.headers.get(\"content-length\", 0))\n",
                "            block_size = 1024\n",
                "            try:\n",
                "                with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=f\"Downloading {file_name}\", disable=not verbose) as pbar:\n",
                "                    with open(path, \"wb\") as f:\n",
                "                        for data in response.iter_content(block_size):\n",
                "                            f.write(data)\n",
                "                            pbar.update(len(data))\n",
                "            except (KeyboardInterrupt, Exception) as e:\n",
                "                os.remove(path)\n",
                "                raise e\n",
                "        decompressed_path = path.replace(\".tar.gz\", \"\")\n",
                "        decompressed_path = decompressed_path.replace(\".gz\", \"\")\n",
                "        if file_name.endswith(\".tar.gz\") and not os.path.exists(f\"{decompressed_path}.tsv\"):\n",
                "            if verbose:\n",
                "                print(f\"Decompressing {file_name}...\")\n",
                "            with tarfile.open(path, \"r:gz\") as tar:\n",
                "                tar.extractall(dataset_dir, filter=\"fully_trusted\")\n",
                "        elif not file_name.endswith(\".tar.gz\") and \\\n",
                "                file_name.endswith(\".gz\") and \\\n",
                "                not os.path.exists(decompressed_path):\n",
                "            if verbose:\n",
                "                print(f\"Decompressing {file_name}...\")\n",
                "            with gzip.open(path, \"rb\") as f_in:\n",
                "                with open(decompressed_path, \"wb\") as f_out:\n",
                "                    f_out.write(f_in.read())\n",
                "\n",
                "def msmarco_dataset_to_contents(corpus: pd.DataFrame, verbose: bool = False) -> SizedGenerator[DocumentContents, None, None]:\n",
                "    \"\"\"\n",
                "    Returns the number of documents and a generator of DocumentContents from the test corpus.\n",
                "    \"\"\"\n",
                "    def inner() -> Generator[DocumentContents, None, None]:\n",
                "        for _, row in corpus.iterrows():\n",
                "            yield DocumentContents(author=\"\", title=\"\", body=row['text'], doc_id=int(row['docno']))\n",
                "    return SizedGenerator(inner(), len(corpus))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "## Download Indices\n",
                "\n",
                "This code defines a function `download_and_extract()` that facilitates downloading and extracting pre-built indices from a given URL.\n",
                "\n",
                "We use it because indexing on Colab wasn't feasible. Our indexing process took about 1 hour, whereas PyTerrier completed it in approximately 15 minutes on a machine equipped with a AMD Ryzen 9 9900X CPU and an NVIDIA GeForce RTX 4070 SUPER GPU.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.utils.download_and_extract ===\n",
                "\n",
                "# import os\n",
                "# import tarfile\n",
                "# import requests\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "def download_and_extract(url: str, path: str, desc: str = \"\"):\n",
                "    stream = requests.get(url, stream=True)\n",
                "    total_size = int(stream.headers.get('content-length', 0))\n",
                "    tgz_path = f\"{path}.tar.gz\"\n",
                "    output_dir = os.path.dirname(path)\n",
                "    if not os.path.exists(tgz_path):\n",
                "        with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024, desc=f\"Downloading {desc}\") as pbar:\n",
                "            with open(tgz_path, 'wb') as f:\n",
                "                for chunk in stream.iter_content(chunk_size=1024):\n",
                "                    f.write(chunk)\n",
                "                    pbar.update(len(chunk))\n",
                "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
                "            members = tqdm([member for member in tar.getmembers() if not os.path.exists(os.path.join(output_dir, member.name))], desc=f\"Extracting {desc}\")\n",
                "            tar.extractall(output_dir, members)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation of the System\n",
                "\n",
                "Now, let's proceed with evaluating our system using the MS-MARCO test-set and compare the results with those obtained from PyTerrier.\n",
                "\n",
                "Note: For inference time reasons, the demo will only run a subset of the test topics. However, we will report the full results at the end of the notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "autoscroll": false
            },
            "outputs": [],
            "source": [
                "#%% === mir.scripts.demo ===\n",
                "\n",
                "# import os\n",
                "import re\n",
                "import pyterrier as pt\n",
                "from pyterrier import IndexFactory\n",
                "# import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# from mir import DATA_DIR\n",
                "# from mir.ir.impls.bm25f_scoring import BM25FScoringFunction\n",
                "# from mir.ir.impls.neural_scoring_function import NeuralScoringFunction\n",
                "# from mir.ir.impls.sqlite_index import SqliteIndex\n",
                "# from mir.ir.ir import Ir\n",
                "# from mir.utils.dataset import get_msmarco_dataset, msmarco_dataset_to_contents\n",
                "# from mir.utils.download_and_extract import download_and_extract\n",
                "\n",
                "\n",
                "get_msmarco_dataset(verbose=True)\n",
                "dataset_csv = f\"{DATA_DIR}/msmarco/collection.tsv\"\n",
                "index_path = f\"{DATA_DIR}/msmarco-pyterrier-index/data.properties\"\n",
                "msmarco_pyterrier_index_url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/msmarco-pyterrier-index.tar.gz\"\n",
                "msmarco_sqlite_index_url = \"https://huggingface.co/Etto48/MIRProject/resolve/main/msmarco-sqlite-index.db.tar.gz\"\n",
                "# download pyterrier index\n",
                "download_and_extract(msmarco_pyterrier_index_url, f\"{DATA_DIR}/msmarco-pyterrier-index\", desc=\"PyTerrier Index\")\n",
                "# download sqlite index\n",
                "download_and_extract(msmarco_sqlite_index_url, f\"{DATA_DIR}/msmarco-sqlite-index.db\", desc=\"SQLite Index\")\n",
                "\n",
                "indexer = pt.terrier.IterDictIndexer(f\"{DATA_DIR}/msmarco-pyterrier-index\")\n",
                "if not os.path.exists(index_path):\n",
                "    dataset = pd.read_csv(dataset_csv, sep='\\t', header=None, names=['docno', 'text'], dtype={'docno': str, 'text': str})\n",
                "    indexref = indexer.index(tqdm(dataset.to_dict(orient='records'), desc=\"Indexing\"))\n",
                "    del dataset\n",
                "else:\n",
                "    indexref = pt.IndexRef.of(index_path)\n",
                "index = IndexFactory.of(indexref)\n",
                "\n",
                "\n",
                "topics_path = f\"{DATA_DIR}/msmarco/msmarco-test2020-queries.tsv\"\n",
                "qrels_path = f\"{DATA_DIR}/msmarco/2020qrels-pass.txt\"\n",
                "\n",
                "topics = pd.read_csv(topics_path, sep='\\t', header=None, names=['qid', 'query'], dtype={'qid': str, 'query': str})\n",
                "qrels = pd.read_csv(qrels_path, sep=' ', header=None, names=['qid', 'Q0', 'docno', 'relevance'], dtype={'qid': str, 'Q0': str, 'docno': str, 'relevance': int})\n",
                "\n",
                "def preprocess_query(query):\n",
                "    query = re.sub(r'[^\\w\\s]', '', query)\n",
                "    query = query.lower()\n",
                "    return query\n",
                "\n",
                "topics['query'] = topics['query'].apply(preprocess_query)\n",
                "\n",
                "my_ir = Ir(SqliteIndex(f\"{DATA_DIR}/msmarco-sqlite-index.db\"), scoring_functions=[\n",
                "    (100, BM25FScoringFunction(1.2, 0.8)),\n",
                "    (10, NeuralScoringFunction())\n",
                "])\n",
                "if len(my_ir.index) == 0:\n",
                "    dataset = pd.read_csv(dataset_csv, sep='\\t', header=None, names=['docno', 'text'], dtype={'docno': str, 'text': str})\n",
                "    sized_generator = msmarco_dataset_to_contents(dataset)\n",
                "    my_ir.bulk_index_documents(sized_generator, verbose=True)\n",
                "\n",
                "my_topics = pd.read_csv(topics_path, sep='\\t', header=None, names=['query_id', 'text'], dtype={'query_id': int, 'text': str})\n",
                "\n",
                "# reduce the number of topics to 10\n",
                "n = 10\n",
                "topics = topics.head(n)\n",
                "qids_str = topics['qid']\n",
                "qids_int = topics['qid'].astype(int)\n",
                "my_topics = my_topics[my_topics['query_id'].isin(qids_int)]\n",
                "qrels = qrels[qrels['qid'].isin(qids_str)]\n",
                "\n",
                "my_run = my_ir.get_run(my_topics, verbose=True, pyterrier_compatible=True)\n",
                "\n",
                "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n",
                "dfree = pt.terrier.Retriever(index, wmodel=\"DFRee\")\n",
                "pyterrier_models = {\n",
                "    \"BM25\": bm25 % 100,\n",
                "    \"BM25+DFRee\": (bm25 % 100) >> dfree\n",
                "}\n",
                "pyterrier_runs = {}\n",
                "for model_name, model in pyterrier_models.items():\n",
                "    print(f\"Running PyTerrier {model_name}\")\n",
                "    pyterrier_runs[model_name] = model.transform(topics)\n",
                "\n",
                "test_runs = [my_run, *pyterrier_runs.values()]\n",
                "names = [\"MyIR\", *pyterrier_models.keys()]\n",
                "\n",
                "metrics = [\"map\", \"ndcg\", \"recip_rank\", \"P.10\", \"recall.10\", ]\n",
                "res = pt.Experiment(test_runs, topics, qrels, metrics, names=names)\n",
                "print(res)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusions\n",
                "\n",
                "Below, we present the benchmark results on the test set:\n",
                "\n",
                "|   |       name |      map |     ndcg | recip_rank |     P.10 | recall.10 |\n",
                "|:--|-----------:|---------:|---------:|-----------:|---------:|----------:|\n",
                "| 0 |       MyIR | 0.310039 | 0.481937 |   0.800661 | 0.566667 |  0.170578 |\n",
                "| 1 |       BM25 | 0.314385 | 0.492467 |   0.802359 | 0.575926 |  0.176116 |\n",
                "| 2 | BM25+DFRee | 0.309455 | 0.491289 |   0.842813 | 0.542593 |  0.174679 |\n",
                "\n",
                "Our Information Retrieval (IR) system demonstrates performance comparable to BM25 provided by PyTerrier. This validates the effectiveness of our approach in addressing large-scale learning-to-rank tasks.\n",
                "\n",
                "A key strength of our system lies in its highly modular architecture. We prioritized designing a flexible and extendable codebase, enabling future enhancements and integration of advanced scoring functions, indexing mechanisms, or re-ranking techniques.\n",
                "\n",
                "### Limitations and Challenges\n",
                "\n",
                "Despite the promising results, our system's inference time lags significantly behind state-of-the-art solutions. This limitation stems from the reliance on less-optimized technologies for handling large datasets and the constrained time frame that limited further optimizations.\n",
                "\n",
                "The primary bottleneck resides in the retrieval of postings and document info from the index implemented with SQL-based technology. While SQL provides robust handling for structured data, it is not optimized for high-throughput IR tasks.\n",
                "\n"
            ]
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 2
}